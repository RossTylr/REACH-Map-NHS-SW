{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc8c040",
   "metadata": {},
   "source": [
    "# Ambulance Station to Hospital Travel Time Analysis\n",
    "\n",
    "This notebook builds a directed bipartite graph connecting ambulance stations to acute hospitals using LSOA-level travel times and fallback estimates. It generates analytics, visualisations, and export artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: install and import packages\n",
    "import sys, subprocess, pkgutil, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def ensure_pkg(pkg):\n",
    "    if pkgutil.find_loader(pkg) is None:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', '-q', 'install', pkg])\n",
    "\n",
    "packages = ['pandas','numpy','networkx','folium','matplotlib','scikit-learn','pyproj','shapely']\n",
    "for p in packages:\n",
    "    ensure_pkg(p)\n",
    "\n",
    "try:\n",
    "    ensure_pkg('torch')\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print('Torch not available:', e)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "DATA_OUT = Path('/mnt/data')\n",
    "DATA_OUT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "DATA_DIR = Path('../data/raw/test_data_ICB_level')\n",
    "stations = pd.read_csv(DATA_DIR / 'ambulance_stations_icb.csv')\n",
    "hospitals = pd.read_csv(DATA_DIR / 'acute_hospitals_icb.csv')\n",
    "matrix = pd.read_csv(DATA_DIR / 'travel_matrix_lsoa_icb.csv')\n",
    "\n",
    "print('Stations shape:', stations.shape)\n",
    "print(stations.head())\n",
    "print('Hospitals shape:', hospitals.shape)\n",
    "print(hospitals.head())\n",
    "print('Matrix shape:', matrix.shape)\n",
    "print(matrix.head())\n",
    "\n",
    "required_station=['Code','Name','latitude','longitude','lsoa21cd']\n",
    "required_hospital=['Code','Name','latitude','longitude','lsoa21cd']\n",
    "required_matrix=['origin_lsoa','dest_lsoa','time_car_min']\n",
    "for req, df, name in [\n",
    "        (required_station, stations, 'stations'),\n",
    "        (required_hospital, hospitals, 'hospitals'),\n",
    "        (required_matrix, matrix, 'matrix')]:\n",
    "    missing=[c for c in req if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name} missing columns: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean & Harmonise\n",
    "stations['lsoa21cd'] = stations['lsoa21cd'].str.strip().str.upper()\n",
    "hospitals['lsoa21cd'] = hospitals['lsoa21cd'].str.strip().str.upper()\n",
    "\n",
    "stations = stations.drop_duplicates('Code').dropna(subset=['latitude','longitude','lsoa21cd'])\n",
    "hospitals = hospitals.drop_duplicates('Code').dropna(subset=['latitude','longitude','lsoa21cd'])\n",
    "\n",
    "if 'icb_code' in stations.columns and 'icb_code' in hospitals.columns:\n",
    "    st_icb = set(stations['icb_code'].unique())\n",
    "    ho_icb = set(hospitals['icb_code'].unique())\n",
    "    print('Station ICB codes:', st_icb)\n",
    "    print('Hospital ICB codes:', ho_icb)\n",
    "    if st_icb != ho_icb:\n",
    "        print('Warning: station and hospital ICB codes differ.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Station-Hospital Pair Table\n",
    "station_cols={'Code':'station_code','Name':'station_name','latitude':'station_lat','longitude':'station_lon','lsoa21cd':'station_lsoa'}\n",
    "hospital_cols={'Code':'hospital_code','Name':'hospital_name','latitude':'hospital_lat','longitude':'hospital_lon','lsoa21cd':'hospital_lsoa'}\n",
    "\n",
    "if 'icb_code' in stations.columns and 'icb_code' in hospitals.columns:\n",
    "    pairs = stations.rename(columns=station_cols).merge(\n",
    "        hospitals.rename(columns=hospital_cols),\n",
    "        on='icb_code', how='outer')\n",
    "else:\n",
    "    stations_tmp = stations.rename(columns=station_cols).assign(key=1)\n",
    "    hospitals_tmp = hospitals.rename(columns=hospital_cols).assign(key=1)\n",
    "    pairs = stations_tmp.merge(hospitals_tmp, on='key').drop('key', axis=1)\n",
    "\n",
    "pairs = pairs.merge(matrix[['origin_lsoa','dest_lsoa','time_car_min']],\n",
    "                    left_on=['station_lsoa','hospital_lsoa'],\n",
    "                    right_on=['origin_lsoa','dest_lsoa'],\n",
    "                    how='left')\n",
    "\n",
    "pairs = pairs.rename(columns={'time_car_min':'time_car_min_official'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback Time Estimator\n",
    "import numpy as np\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "pairs['dist_km'] = haversine_km(pairs['station_lat'], pairs['station_lon'],\n",
    "                               pairs['hospital_lat'], pairs['hospital_lon'])\n",
    "pairs['time_car_min_fallback'] = np.where(pairs['time_car_min_official'].isna(),\n",
    "                                         (pairs['dist_km']/50.0*60.0)*1.3,\n",
    "                                         np.nan)\n",
    "pairs['time_min'] = pairs['time_car_min_official'].fillna(pairs['time_car_min_fallback'])\n",
    "pairs['has_official_time'] = ~pairs['time_car_min_official'].isna()\n",
    "\n",
    "total = len(pairs)\n",
    "official = pairs['has_official_time'].sum()\n",
    "fallback = total - official\n",
    "print(f'Total pairs: {total}; official: {official} ({official/total*100:.1f}%), '\n",
    "      f'fallback: {fallback} ({fallback/total*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaee844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Graph (NetworkX)\n",
    "G = nx.DiGraph()\n",
    "for _, r in stations.iterrows():\n",
    "    G.add_node(r['Code'], type='station', code=r['Code'], name=r['Name'],\n",
    "               latitude=r['latitude'], longitude=r['longitude'], lsoa=r['lsoa21cd'])\n",
    "for _, r in hospitals.iterrows():\n",
    "    G.add_node(r['Code'], type='hospital', code=r['Code'], name=r['Name'],\n",
    "               latitude=r['latitude'], longitude=r['longitude'], lsoa=r['lsoa21cd'])\n",
    "for _, r in pairs.iterrows():\n",
    "    G.add_edge(r['station_code'], r['hospital_code'],\n",
    "               time_min=r['time_min'], has_official_time=r['has_official_time'],\n",
    "               origin_lsoa=r['station_lsoa'], dest_lsoa=r['hospital_lsoa'])\n",
    "print('Graph nodes:', G.number_of_nodes(), 'edges:', G.number_of_edges())\n",
    "isolated = [n for n, d in G.nodes(data=True) if d['type']=='station' and G.out_degree(n)==0]\n",
    "if isolated:\n",
    "    print('Warning: isolated stations:', isolated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytics\n",
    "pairs_sorted = pairs.sort_values(['station_code','time_min'])\n",
    "nearest = pairs_sorted.groupby('station_code').first().reset_index()\n",
    "top3 = pairs_sorted.groupby('station_code').head(3)\n",
    "\n",
    "thresholds = [10,20,30]\n",
    "coverage = {}\n",
    "for t in thresholds:\n",
    "    coverage[t] = pairs[pairs['time_min']<=t].groupby('station_code').size()\n",
    "coverage_df = pd.DataFrame(coverage).fillna(0).astype(int)\n",
    "print('Coverage by threshold (counts per station):')\n",
    "print(coverage_df.describe())\n",
    "\n",
    "weights = pairs.assign(weight=1/(1+pairs['time_min']))\n",
    "centrality = weights.groupby('hospital_code')['weight'].sum().sort_values(ascending=False)\n",
    "print()\n",
    "print('Top hospitals by in-strength:')\n",
    "print(centrality.head())\n",
    "\n",
    "assignment = nearest[['station_code','station_name','hospital_code','hospital_name','time_min']]\n",
    "print()\n",
    "print('Sample stationâ†’best hospital:')\n",
    "print(assignment.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f4036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation (Folium)\n",
    "map_center = [pairs['station_lat'].mean(), pairs['station_lon'].mean()]\n",
    "m = folium.Map(location=map_center, zoom_start=8)\n",
    "for _, r in stations.iterrows():\n",
    "    folium.CircleMarker(location=[r['latitude'], r['longitude']], radius=4,\n",
    "                        color='blue', fill=True, fill_opacity=0.7,\n",
    "                        popup=r['Name']).add_to(m)\n",
    "for _, r in hospitals.iterrows():\n",
    "    folium.CircleMarker(location=[r['latitude'], r['longitude']], radius=4,\n",
    "                        color='red', fill=True, fill_opacity=0.7,\n",
    "                        popup=r['Name']).add_to(m)\n",
    "nearest_geo = nearest.merge(stations[['Code','latitude','longitude']],\n",
    "                            left_on='station_code', right_on='Code')                      .merge(hospitals[['Code','latitude','longitude']],\n",
    "                            left_on='hospital_code', right_on='Code',\n",
    "                            suffixes=('_station','_hospital'))\n",
    "for _, r in nearest_geo.iterrows():\n",
    "    folium.PolyLine(locations=[[r['latitude_station'], r['longitude_station']],\n",
    "                               [r['latitude_hospital'], r['longitude_hospital']]],\n",
    "                    color='gray', weight=1).add_to(m)\n",
    "map_path = str(Path('/mnt/data/station_hospital_map.html').resolve())\n",
    "m.save(map_path)\n",
    "print('Map saved to', map_path)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a46ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports\n",
    "nodes_df = pd.concat([\n",
    "    stations[['Code','Name','latitude','longitude','lsoa21cd']].assign(type='station'),\n",
    "    hospitals[['Code','Name','latitude','longitude','lsoa21cd']].assign(type='hospital')\n",
    "])\n",
    "nodes_df['id'] = nodes_df['Code']\n",
    "nodes_df = nodes_df.rename(columns={'Code':'code','Name':'name','lsoa21cd':'lsoa'})\n",
    "nodes_df = nodes_df[['id','type','code','name','latitude','longitude','lsoa']]\n",
    "nodes_path = str(Path('/mnt/data/nodes_station_hospital.csv').resolve())\n",
    "nodes_df.to_csv(nodes_path, index=False)\n",
    "\n",
    "edges_df = pairs[['station_code','hospital_code','time_min','has_official_time','station_lsoa','hospital_lsoa']]     .rename(columns={'station_code':'source_code','hospital_code':'target_code',\n",
    "                     'station_lsoa':'origin_lsoa','hospital_lsoa':'dest_lsoa'})\n",
    "edges_path = str(Path('/mnt/data/edges_station_to_hospital.csv').resolve())\n",
    "edges_df.to_csv(edges_path, index=False)\n",
    "\n",
    "graphml_path = str(Path('/mnt/data/station_hospital.graphml').resolve())\n",
    "nx.write_graphml(G, graphml_path)\n",
    "\n",
    "assignment_path = str(Path('/mnt/data/station_best_hospital.csv').resolve())\n",
    "assignment.to_csv(assignment_path, index=False)\n",
    "\n",
    "print('Exports written:')\n",
    "for p in [nodes_path, edges_path, graphml_path, assignment_path]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00829088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Tiny Neural-Net Baseline\n",
    "if TORCH_AVAILABLE:\n",
    "    pairs_nn = pairs.copy()\n",
    "    pairs_nn['delta_lat'] = pairs_nn['hospital_lat'] - pairs_nn['station_lat']\n",
    "    pairs_nn['delta_lon'] = pairs_nn['hospital_lon'] - pairs_nn['station_lon']\n",
    "    feature_cols = ['dist_km','delta_lat','delta_lon','station_lat','station_lon','hospital_lat','hospital_lon']\n",
    "    X = pairs_nn[feature_cols].values\n",
    "    y = pairs_nn['time_min'].values\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(len(feature_cols),16),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(16,8),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(8,1)\n",
    "    )\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_val)\n",
    "    mae = torch.mean(torch.abs(val_pred - y_val)).item()\n",
    "    ss_res = torch.sum((y_val - val_pred)**2)\n",
    "    ss_tot = torch.sum((y_val - torch.mean(y_val))**2)\n",
    "    r2 = 1 - ss_res/ss_tot\n",
    "    print(f'Validation MAE: {mae:.2f} min, R^2: {r2:.2f}')\n",
    "    plt.figure()\n",
    "    plt.scatter(y_val.numpy(), (val_pred - y_val).numpy())\n",
    "    plt.axhline(0, color='red')\n",
    "    plt.xlabel('True time')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title('Residual plot')\n",
    "    plt.show()\n",
    "    import pickle\n",
    "    scaler_path = str(Path('/mnt/data/time_model_scaler.pkl').resolve())\n",
    "    model_path = str(Path('/mnt/data/time_mlp.pt').resolve())\n",
    "    with open(scaler_path,'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('Saved:', scaler_path, model_path)\n",
    "else:\n",
    "    print('Skipping neural net section; torch not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849dc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Checks & Summary\n",
    "print(f'Total nodes: {G.number_of_nodes()}')\n",
    "print(f'Total edges: {G.number_of_edges()}')\n",
    "print(f'Official coverage: {official/total*100:.1f}% of pairs')\n",
    "print('Outputs:')\n",
    "print(map_path)\n",
    "print(nodes_path)\n",
    "print(edges_path)\n",
    "print(graphml_path)\n",
    "print(assignment_path)\n",
    "if TORCH_AVAILABLE:\n",
    "    print(scaler_path)\n",
    "    print(model_path)\n",
    "print('Note: LSOA-based times approximate real routes; fallback estimates are heuristic.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
