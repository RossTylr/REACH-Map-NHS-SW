{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22eb63e9-4ab9-4b07-9a15-fc29a7ca561d",
   "metadata": {},
   "source": [
    "### 02b_matrix_coverage_fast_ops — end-to-end, sparse-matrix engine \n",
    "\n",
    "**Notebook purpose (plain language)**  \n",
    "This notebook swaps the *engine* that finds minimum times in `02a_coverage`.  \n",
    "Instead of pandas group-bys on the long travel table, we build two *sparse* matrices and use fast, vectorised reductions. All inputs, thresholds, blue-light factors, KPIs, and maps stay the same—only the internals get faster and more scalable, enabling instant “what-if” scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "#### What this notebook does\n",
    "\n",
    "- **Builds matrices (reshape only, no routing):**  \n",
    "  - **R** (response): rows = demand LSOAs, cols = station LSOAs, values = minutes station→LSOA.  \n",
    "  - **C** (conveyance): rows = demand LSOAs, cols = acute LSOAs, values = minutes LSOA→acute.\n",
    "- **Computes nearest times (vectorised, no loops):**  \n",
    "  - `t_resp = rowwise_min(R[:, active_stations])`  \n",
    "  - `t_conv = rowwise_min(C[:, active_acutes])`\n",
    "- **Applies business rules:** blue-light factors applied *after* minima (per-leg), thresholds as in 02a.\n",
    "- **Outputs unchanged:** coverage KPIs (% pop within 7/15 and 18/40), binary coverage columns, maps.\n",
    "- **Adds optional diagnostic:** `t_total = t_resp + on_scene_buffer + t_conv` (three-leg view).\n",
    "- **Enables scenarios:** select different station sets by column subset—no re-grouping or re-reading.\n",
    "\n",
    "---\n",
    "\n",
    "#### Inputs (same as 02a)\n",
    "\n",
    "- LSOA universe (`lsoa_index`), centroids, populations (+ optional IMD / rural-urban).  \n",
    "- Station & acute site files resolved to LSOA codes.  \n",
    "- Long-form travel-time table already in the repo (response & conveyance legs).  \n",
    "- Thresholds & blue-light factors (ARP, handover, conveyance) defined up front.\n",
    "\n",
    "---\n",
    "\n",
    "#### Outputs\n",
    "\n",
    "- KPIs for response & conveyance at configured thresholds (overall and, optionally, by IMD/rural-urban).  \n",
    "- Binary coverage columns per threshold for mapping.  \n",
    "- (Optional) End-to-end time columns for transparency in pathway discussions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Performance & storage\n",
    "\n",
    "- **Sparse CSR** matrices for R and C; optional “has-edge” masks to distinguish true zeros from missing pairs.  \n",
    "- **Radius thinning** (e.g., drop times > 60 min) to shrink matrices without losing feasible options.  \n",
    "- **Caching:** save matrices + ordered labels to `data/.../matrices/*.npz` for instant reloads.\n",
    "\n",
    "---\n",
    "\n",
    "#### Scenario selector\n",
    "\n",
    "Define scenarios as sets of active station/acute columns (e.g., `baseline`, `baseline + Site X`).  \n",
    "Switching scenario = re-taking per-row minima → near-instant “what-if” diffs and coverage deltas.\n",
    "\n",
    "---\n",
    "\n",
    "#### Validation (first run)\n",
    "\n",
    "- **Parity check vs 02a:** times and KPIs should match within tight tolerance on the Cornwall slice.  \n",
    "- After validation, retire the legacy `min_time_from_any_origin` calls in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "#### Notes & cautions\n",
    "\n",
    "- Any change to travel-time inputs **invalidates caches** → rebuild matrices.  \n",
    "- LSOAs with no reachable station/acute remain at **∞** and are reported explicitly.  \n",
    "- Keep LSOA codes categorical and ordering stable to avoid misalignment bugs.\n",
    "\n",
    "---\n",
    "\n",
    "#### Quick explainer (02a → 02b)\n",
    "\n",
    "| Area / Step | 02a does now | 02b change | Benefit |\n",
    "|---|---|---|---|\n",
    "| Min times | pandas group-by on long table | rowwise min on sparse matrices | Much faster; scalable; no loops |\n",
    "| Scenarios | re-filter + re-group | column subset on R/C | Instant “what-if” |\n",
    "| Blue-light | applied in KPIs | apply after minima per leg | Correct nearest selection |\n",
    "| Outputs | KPIs, maps | same | No UX change |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df6d6bba-b68b-472d-bdc8-8169d53289a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 — Imports, params (updated: ARP + handover thresholds)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = Path(\n",
    "    \"/Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/\"\n",
    "    \"GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level\"\n",
    ")\n",
    "\n",
    "# Files\n",
    "LOOKUP_CSV   = DATA_ROOT / \"cornwall_icb_lsoa_lookup.csv\"\n",
    "AGE_GPKG     = DATA_ROOT / \"demographics_age_continuous_icb.gpkg\"\n",
    "AGE_LAYER    = \"LSOA_continuous_age_icb\"\n",
    "TRAVEL_CSV   = DATA_ROOT / \"travel_matrix_lsoa_icb.csv\"\n",
    "STATIONS_CSV = DATA_ROOT / \"ambulance_stations_icb.csv\"\n",
    "ACUTE_CSV    = DATA_ROOT / \"acute_hospitals_icb.csv\"   # overlay optional\n",
    "\n",
    "# --- Targets / thresholds ----------------------------------------------------\n",
    "# ARP response standards (minutes)\n",
    "RESP = {\n",
    "    \"cat1\": {\"mean\": 7,  \"p90\": 15},\n",
    "    \"cat2\": {\"mean\": 18, \"p90\": 40},\n",
    "    # optional for completeness:\n",
    "    \"cat3\": {\"p90\": 120},\n",
    "    \"cat4\": {\"p90\": 180},\n",
    "}\n",
    "# Convenience tuple for KPI lookups used in this notebook\n",
    "RESPONSE_THRESHOLDS = (RESP[\"cat1\"][\"mean\"], RESP[\"cat1\"][\"p90\"],\n",
    "                       RESP[\"cat2\"][\"mean\"], RESP[\"cat2\"][\"p90\"])  # -> (7, 15, 18, 40)\n",
    "\n",
    "# Hospital handover/turnaround (NOT scene→A&E drive time)\n",
    "HANDOVER = {\"target\": 15, \"breach\": 30, \"severe\": 60}\n",
    "HANDOVER_THRESHOLDS = (HANDOVER[\"target\"], HANDOVER[\"breach\"], HANDOVER[\"severe\"])  # (15, 30, 60)\n",
    "\n",
    "# Scene→A&E conveyance (geographic potential only; no national target)\n",
    "# Keep if you want to map the drive leg; adjust locally if desired.\n",
    "SCENE_TO_AE_THRESHOLDS = (30, 45, 60)\n",
    "\n",
    "# Optional blue-light factors applied to car travel times (1.0 = off)\n",
    "BLUE_LIGHT_FACTOR_RESPONSE = 1.0\n",
    "BLUE_LIGHT_FACTOR_CONVEY   = 1.0\n",
    "\n",
    "# Display prefs\n",
    "pd.set_option(\"display.width\", 120)\n",
    "pd.set_option(\"display.max_columns\", 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47737468-a9a0-4f6e-bf34-e0bc7d131500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Universe ready: 336 LSOAs\n",
      "[OK] Population loaded (sum=575,628; non-zero LSOAs=336); CRS=EPSG:27700\n",
      "[WARN] Floored 41 off-diagonal zero-minute rows to 0.5 min.\n",
      "[OK] Travel rows: 112,560 (was 112,560); origins=336; dests=336\n",
      "[OK] Stations mapped to 14 LSOAs; Acutes mapped to 3 LSOAs\n",
      "[OK] Diagonal 0-min rows kept: 0 (ALLOW_DIAGONAL_ZERO=True)\n",
      "\n",
      "== STEP 1 SUMMARY (cleaned) ==\n",
      "n_lsoas                 336\n",
      "population_sum       575628\n",
      "travel_rows          112560\n",
      "unique_origins          336\n",
      "unique_dests            336\n",
      "zeros_diag                0\n",
      "zeros_offdiag             0\n",
      "n_station_lsoas          14\n",
      "n_acute_lsoas             3\n",
      "station_idx_dtype     int32\n",
      "acute_idx_dtype       int32\n",
      "[OK] Step 1 complete — data aligned, cleaned, and inspected. Ready for Step 2 (sparse matrices).\n"
     ]
    }
   ],
   "source": [
    "# Step 1 — Load, inspect, and clean core data (universe, population, travel, sites)\n",
    "# (Refined based on your Step 1 output: 336 LSOAs, 112,560 OD rows, 14 station LSOAs, 3 acute LSOAs)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Iterable, Dict\n",
    "import re\n",
    "\n",
    "# ---------- configurable cleaning rules (tweak if needed) ----------\n",
    "ALLOW_DIAGONAL_ZERO = True     # keep 0 min where origin_lsoa == dest_lsoa\n",
    "FLOOR_OFFDIAG_ZERO_MIN = 0.5   # minutes; applied only if origin != dest and time <= 0\n",
    "DROP_NEGATIVES = True          # drop rows with time < 0 (data error)\n",
    "MAX_SANITY_MIN = 300           # flag values > this as outliers (warn only)\n",
    "\n",
    "# ---------- small helpers ----------\n",
    "def _ok(msg: str) -> None:\n",
    "    print(f\"[OK] {msg}\")\n",
    "\n",
    "def _warn(msg: str) -> None:\n",
    "    print(f\"[WARN] {msg}\")\n",
    "\n",
    "def _fail(msg: str) -> None:\n",
    "    raise AssertionError(msg)\n",
    "\n",
    "def _expect_columns(df: pd.DataFrame, cols: Iterable[str], label: str) -> None:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        _fail(f\"{label}: missing columns {missing}\")\n",
    "\n",
    "# ---------- 1) LSOA universe ----------\n",
    "lookup = pd.read_csv(LOOKUP_CSV, dtype={\"lsoa_code\": \"string\"})\n",
    "_expect_columns(lookup, [\"lsoa_code\"], \"LSOA lookup\")\n",
    "lookup = lookup.drop_duplicates(subset=[\"lsoa_code\"]).copy()\n",
    "\n",
    "lsoa_index = pd.Index(lookup[\"lsoa_code\"].astype(\"string\"), name=\"lsoa_code\")\n",
    "if lsoa_index.empty or not lsoa_index.is_unique:\n",
    "    _fail(\"LSOA lookup must provide a non-empty, unique list of LSOA codes.\")\n",
    "_ok(f\"Universe ready: {len(lsoa_index):,} LSOAs\")\n",
    "\n",
    "# ---------- 2) Population & geometry ----------\n",
    "lsoa_g = gpd.read_file(AGE_GPKG, layer=AGE_LAYER)\n",
    "_expect_columns(lsoa_g, [\"lsoa_code\", \"geometry\"], \"Age GPKG\")\n",
    "lsoa_g[\"lsoa_code\"] = lsoa_g[\"lsoa_code\"].astype(\"string\")\n",
    "\n",
    "# Determine population: prefer population_total/population; else sum continuous ages\n",
    "pop_col = next((c for c in (\"population_total\", \"population\") if c in lsoa_g.columns), None)\n",
    "if pop_col:\n",
    "    population = lsoa_g.set_index(\"lsoa_code\")[pop_col].astype(\"float64\")\n",
    "else:\n",
    "    age_cols = []\n",
    "    for c in lsoa_g.columns:\n",
    "        if c in (\"lsoa_code\", \"geometry\"):\n",
    "            continue\n",
    "        if (re.fullmatch(r\"\\d{1,3}\\+?\", str(c)) or str(c).startswith(\"age_\")) and np.issubdtype(lsoa_g[c].dtype, np.number):\n",
    "            age_cols.append(c)\n",
    "    if not age_cols:\n",
    "        _fail(\"Could not infer population: no 'population_total' nor numeric continuous age columns found.\")\n",
    "    population = lsoa_g.set_index(\"lsoa_code\")[age_cols].sum(axis=1).astype(\"float64\")\n",
    "\n",
    "# Align to universe\n",
    "population = population.reindex(lsoa_index).fillna(0.0)\n",
    "lsoa_g = (\n",
    "    lsoa_g[[\"lsoa_code\", \"geometry\"]]\n",
    "    .drop_duplicates(\"lsoa_code\")\n",
    "    .set_index(\"lsoa_code\")\n",
    "    .reindex(lsoa_index)\n",
    ")\n",
    "lsoa_g = gpd.GeoDataFrame(lsoa_g, geometry=\"geometry\", crs=lsoa_g.crs)\n",
    "\n",
    "_ok(f\"Population loaded (sum={int(population.sum()):,}; non-zero LSOAs={(population > 0).sum():,}); CRS={lsoa_g.crs}\")\n",
    "\n",
    "# ---------- 3) Travel table (load → inspect → clean) ----------\n",
    "travel = pd.read_csv(\n",
    "    TRAVEL_CSV,\n",
    "    dtype={\"origin_lsoa\": \"string\", \"dest_lsoa\": \"string\"},\n",
    ")\n",
    "\n",
    "# Normalise time column to 'time_car_min'\n",
    "time_col = next((c for c in (\"time_car_min\", \"time_min\", \"minutes\", \"drive_min\", \"t_min\") if c in travel.columns), None)\n",
    "if time_col is None:\n",
    "    _fail(\"Travel CSV must include a minutes column (e.g., 'time_car_min' / 'time_min' / 'minutes').\")\n",
    "\n",
    "travel = travel.rename(columns={time_col: \"time_car_min\"})\n",
    "_expect_columns(travel, [\"origin_lsoa\", \"dest_lsoa\", \"time_car_min\"], \"Travel CSV\")\n",
    "\n",
    "# Type and NA clean\n",
    "travel[\"origin_lsoa\"] = travel[\"origin_lsoa\"].astype(\"string\")\n",
    "travel[\"dest_lsoa\"] = travel[\"dest_lsoa\"].astype(\"string\")\n",
    "travel[\"time_car_min\"] = pd.to_numeric(travel[\"time_car_min\"], errors=\"coerce\").astype(\"float32\")\n",
    "travel = travel.dropna(subset=[\"origin_lsoa\", \"dest_lsoa\", \"time_car_min\"]).copy()\n",
    "\n",
    "# Keep only rows within the LSOA universe (both origin and dest)\n",
    "in_universe = travel[\"origin_lsoa\"].isin(lsoa_index) & travel[\"dest_lsoa\"].isin(lsoa_index)\n",
    "dropped_outside = int((~in_universe).sum())\n",
    "if dropped_outside:\n",
    "    _warn(f\"Dropping {dropped_outside:,} travel rows outside LSOA universe.\")\n",
    "travel = travel.loc[in_universe].copy()\n",
    "\n",
    "# Inspect problematic times\n",
    "is_diag = travel[\"origin_lsoa\"] == travel[\"dest_lsoa\"]\n",
    "is_zero = travel[\"time_car_min\"] == 0\n",
    "is_neg = travel[\"time_car_min\"] < 0\n",
    "is_large = travel[\"time_car_min\"] > MAX_SANITY_MIN\n",
    "offdiag_zero = (~is_diag) & is_zero\n",
    "\n",
    "n_rows0 = len(travel)\n",
    "n_neg = int(is_neg.sum())\n",
    "n_zero_diag = int((is_diag & is_zero).sum())\n",
    "n_zero_offdiag = int(offdiag_zero.sum())\n",
    "n_large = int(is_large.sum())\n",
    "\n",
    "# Clean according to rules\n",
    "if DROP_NEGATIVES and n_neg:\n",
    "    travel = travel.loc[~is_neg].copy()\n",
    "    _warn(f\"Dropped {n_neg:,} rows with negative minutes.\")\n",
    "\n",
    "if n_zero_offdiag:\n",
    "    # Set small positive floor for off-diagonal zeros\n",
    "    travel.loc[offdiag_zero, \"time_car_min\"] = np.float32(FLOOR_OFFDIAG_ZERO_MIN)\n",
    "    _warn(f\"Floored {n_zero_offdiag:,} off-diagonal zero-minute rows to {FLOOR_OFFDIAG_ZERO_MIN} min.\")\n",
    "\n",
    "if not ALLOW_DIAGONAL_ZERO and n_zero_diag:\n",
    "    travel.loc[is_diag & is_zero, \"time_car_min\"] = np.float32(FLOOR_OFFDIAG_ZERO_MIN)\n",
    "    _warn(f\"Replaced {n_zero_diag:,} diagonal zeros with {FLOOR_OFFDIAG_ZERO_MIN} min (ALLOW_DIAGONAL_ZERO=False).\")\n",
    "\n",
    "# Recompute quick stats post-clean\n",
    "n_rows1 = len(travel)\n",
    "_ok(\n",
    "    f\"Travel rows: {n_rows1:,} (was {n_rows0:,}); \"\n",
    "    f\"origins={travel['origin_lsoa'].nunique():,}; dests={travel['dest_lsoa'].nunique():,}\"\n",
    ")\n",
    "if n_large:\n",
    "    _warn(f\"{n_large:,} rows have very large minutes (>{MAX_SANITY_MIN}); keep but flagged.\")\n",
    "\n",
    "# Coverage against universe (warn-only)\n",
    "orig_set = pd.Index(travel[\"origin_lsoa\"].unique(), dtype=\"string\")\n",
    "dest_set = pd.Index(travel[\"dest_lsoa\"].unique(), dtype=\"string\")\n",
    "miss_orig = lsoa_index.difference(orig_set)\n",
    "miss_dest = lsoa_index.difference(dest_set)\n",
    "if len(miss_orig):\n",
    "    _warn(f\"{len(miss_orig)} LSOAs absent as origins (e.g., {list(miss_orig[:5])})\")\n",
    "if len(miss_dest):\n",
    "    _warn(f\"{len(miss_dest)} LSOAs absent as destinations (e.g., {list(miss_dest[:5])})\")\n",
    "\n",
    "# ---------- 4) Sites (stations & acutes resolved to LSOA) ----------\n",
    "def _load_site_lsoas(csv_path: Path, label: str) -> pd.Index:\n",
    "    if not csv_path.exists():\n",
    "        if label.lower().startswith(\"acute\"):\n",
    "            _warn(\"Acute CSV not found; conveyance leg will be optional.\")\n",
    "            return pd.Index([], dtype=\"string\", name=\"lsoa_code\")\n",
    "        _fail(f\"{label} CSV not found: {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    code_col = next((c for c in (\"lsoa_code\", \"lsoa21cd\") if c in df.columns), None)\n",
    "    if code_col is None:\n",
    "        _fail(f\"{label}: expected an LSOA code column ('lsoa_code' or 'lsoa21cd').\")\n",
    "    codes = pd.Index(df[code_col].astype(\"string\"), name=\"lsoa_code\").dropna().drop_duplicates()\n",
    "    codes = codes[codes.isin(lsoa_index)]\n",
    "    if codes.empty:\n",
    "        _warn(f\"{label}: no valid LSOAs after filtering to universe.\")\n",
    "    return codes\n",
    "\n",
    "station_lsoas = _load_site_lsoas(STATIONS_CSV, \"Ambulance stations\")\n",
    "acute_lsoas   = _load_site_lsoas(ACUTE_CSV,    \"Acute hospitals\")\n",
    "\n",
    "_ok(f\"Stations mapped to {len(station_lsoas):,} LSOAs; Acutes mapped to {len(acute_lsoas):,} LSOAs\")\n",
    "\n",
    "# Ensure coverage in travel table for sites we’ll need later\n",
    "missing_station_as_origin = station_lsoas.difference(orig_set)\n",
    "missing_station_as_dest   = station_lsoas.difference(dest_set)  # may be irrelevant for response\n",
    "if len(missing_station_as_origin):\n",
    "    _warn(f\"{len(missing_station_as_origin)} station LSOAs not present as travel origins \"\n",
    "          f\"(e.g., {list(missing_station_as_origin[:5])}) — check travel construction.\")\n",
    "missing_acute_as_dest = acute_lsoas.difference(dest_set)\n",
    "if len(missing_acute_as_dest):\n",
    "    _warn(f\"{len(missing_acute_as_dest)} acute LSOAs not present as travel destinations \"\n",
    "          f\"(e.g., {list(missing_acute_as_dest[:5])}) — check travel construction.\")\n",
    "\n",
    "# ---------- 5) Integer mappings for matrices ----------\n",
    "lsoa_to_idx: Dict[str, int] = {code: i for i, code in enumerate(lsoa_index)}\n",
    "idx_to_lsoa = lsoa_index.to_numpy()\n",
    "\n",
    "station_idx = np.array([lsoa_to_idx[c] for c in station_lsoas], dtype=np.int32) if len(station_lsoas) else np.array([], dtype=np.int32)\n",
    "acute_idx   = np.array([lsoa_to_idx[c] for c in acute_lsoas],   dtype=np.int32) if len(acute_lsoas)   else np.array([], dtype=np.int32)\n",
    "\n",
    "# ---------- 6) Report diagonals & zero counts explicitly ----------\n",
    "diag_zero_ct = int(((travel[\"origin_lsoa\"] == travel[\"dest_lsoa\"]) & (travel[\"time_car_min\"] == 0)).sum())\n",
    "offdiag_zero_ct = int(((travel[\"origin_lsoa\"] != travel[\"dest_lsoa\"]) & (travel[\"time_car_min\"] == 0)).sum())\n",
    "_ok(f\"Diagonal 0-min rows kept: {diag_zero_ct:,} (ALLOW_DIAGONAL_ZERO={ALLOW_DIAGONAL_ZERO})\")\n",
    "if offdiag_zero_ct:\n",
    "    _warn(f\"Off-diagonal 0-min rows remaining after cleaning: {offdiag_zero_ct:,}\")\n",
    "\n",
    "# ---------- 7) Step 1 summary ----------\n",
    "summary = pd.Series(\n",
    "    {\n",
    "        \"n_lsoas\": len(lsoa_index),\n",
    "        \"population_sum\": int(population.sum()),\n",
    "        \"travel_rows\": len(travel),\n",
    "        \"unique_origins\": travel[\"origin_lsoa\"].nunique(),\n",
    "        \"unique_dests\": travel[\"dest_lsoa\"].nunique(),\n",
    "        \"zeros_diag\": diag_zero_ct,\n",
    "        \"zeros_offdiag\": offdiag_zero_ct,\n",
    "        \"n_station_lsoas\": len(station_lsoas),\n",
    "        \"n_acute_lsoas\": len(acute_lsoas),\n",
    "        \"station_idx_dtype\": str(station_idx.dtype),\n",
    "        \"acute_idx_dtype\": str(acute_idx.dtype),\n",
    "    }\n",
    ")\n",
    "print(\"\\n== STEP 1 SUMMARY (cleaned) ==\")\n",
    "print(summary.to_string())\n",
    "\n",
    "_ok(\"Step 1 complete — data aligned, cleaned, and inspected. Ready for Step 2 (sparse matrices).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dff90484-88df-4c8f-9262-f0fd27841086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] R (station→LSOA): shape=(336, 14), nnz=4,690, density=0.9970\n",
      "[OK] C (LSOA→acute): shape=(336, 3), nnz=1,005, density=0.9970\n",
      "[OK] Cached matrices to: /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/matrices\n",
      "t_resp (min to any station): min=0.5, p25=5.7, median=11.22, p90=24.64, p95=28.76, max=92.22 (n=336)\n",
      "t_conv (min to any acute): min=0.5, p25=16.95, median=29.28, p90=74.47, p95=80.82, max=129.12 (n=336)\n",
      "Response coverage (all stations): ≤7 min:  30.8% | ≤15 min:  62.3% | ≤18 min:  75.2% | ≤40 min:  99.3%\n",
      "Conveyance coverage (all acutes): ≤30 min:  51.8% | ≤45 min:  69.4% | ≤60 min:  79.0%\n",
      "[OK] Step 2 complete — matrices built, cached, and baseline sanity computed.\n"
     ]
    }
   ],
   "source": [
    "# Step 2 — Build & cache sparse matrices (R: station→LSOA, C: LSOA→acute)\n",
    "# Produces CSR matrices + boolean masks and saves them to MATRICES_DIR.\n",
    "# Also prints a quick baseline sanity check (nearest times & coverage bands).\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Tuple, Sequence, Dict\n",
    "from scipy import sparse\n",
    "\n",
    "# Optional radius thinning (minutes). Set to None to keep all pairs.\n",
    "MAX_RADIUS_MIN: float | None = None  # e.g., 60.0 to thin, or None to keep all\n",
    "\n",
    "\n",
    "def _build_sparse_matrix(\n",
    "    travel_df: pd.DataFrame,\n",
    "    row_codes: pd.Index,            # demand LSOAs (rows)\n",
    "    col_codes: pd.Index,            # site LSOAs for this leg (columns)\n",
    "    row_key: str,                   # travel column holding row codes\n",
    "    col_key: str,                   # travel column holding col codes\n",
    "    value_key: str = \"time_car_min\",\n",
    "    max_radius: float | None = MAX_RADIUS_MIN,\n",
    ") -> Tuple[sparse.csr_matrix, sparse.csr_matrix]:\n",
    "    \"\"\"\n",
    "    Reshape long travel table into a CSR matrix of minutes plus a boolean mask CSR\n",
    "    with identical sparsity (1=has edge). Rows are row_codes, columns are col_codes.\n",
    "    \"\"\"\n",
    "    if len(col_codes) == 0:\n",
    "        # Empty column set → return empty (n_rows x 0) matrices\n",
    "        shape = (len(row_codes), 0)\n",
    "        return sparse.csr_matrix(shape, dtype=np.float32), sparse.csr_matrix(shape, dtype=np.uint8)\n",
    "\n",
    "    # Fast membership filters\n",
    "    idx_row = row_codes.astype(\"string\")\n",
    "    idx_col = col_codes.astype(\"string\")\n",
    "    m = travel_df[row_key].isin(idx_row) & travel_df[col_key].isin(idx_col)\n",
    "\n",
    "    df = travel_df.loc[m, [row_key, col_key, value_key]].dropna(subset=[value_key]).copy()\n",
    "\n",
    "    # Optional radius thinning\n",
    "    if max_radius is not None:\n",
    "        df = df.loc[df[value_key] <= float(max_radius)].copy()\n",
    "\n",
    "    # Group duplicates to their minimum (defensive)\n",
    "    df = (\n",
    "        df.groupby([row_key, col_key], observed=True, sort=False)[value_key]\n",
    "        .min()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Build integer lookups for rows/cols local to this matrix\n",
    "    row_lookup: Dict[str, int] = {code: i for i, code in enumerate(idx_row)}\n",
    "    col_lookup: Dict[str, int] = {code: j for j, code in enumerate(idx_col)}\n",
    "\n",
    "    rows = df[row_key].map(row_lookup).to_numpy(dtype=np.int32, na_value=-1)\n",
    "    cols = df[col_key].map(col_lookup).to_numpy(dtype=np.int32, na_value=-1)\n",
    "    vals = df[value_key].astype(np.float32).to_numpy()\n",
    "\n",
    "    # Drop any pairs that mapped to -1 (should not happen given filters)\n",
    "    good = (rows >= 0) & (cols >= 0) & np.isfinite(vals)\n",
    "    rows, cols, vals = rows[good], cols[good], vals[good]\n",
    "\n",
    "    shape = (len(row_codes), len(col_codes))\n",
    "    coo = sparse.coo_matrix((vals, (rows, cols)), shape=shape, dtype=np.float32)\n",
    "    mat = coo.tocsr()\n",
    "\n",
    "    # Boolean mask with identical sparsity (1 where an edge exists)\n",
    "    mask = sparse.csr_matrix((np.ones_like(mat.data, dtype=np.uint8), mat.indices, mat.indptr), shape=mat.shape)\n",
    "\n",
    "    return mat, mask\n",
    "\n",
    "\n",
    "def _rowwise_min_with_mask(\n",
    "    mat: sparse.csr_matrix,\n",
    "    mask: sparse.csr_matrix,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute row-wise minima while treating 'no edge' as +inf (instead of 0).\n",
    "    Returns a float32 dense vector of shape (n_rows,).\n",
    "    \"\"\"\n",
    "    if mat.shape[1] == 0:\n",
    "        return np.full(mat.shape[0], np.inf, dtype=np.float32)\n",
    "    arr = mat.toarray()\n",
    "    arr_mask = mask.toarray().astype(bool)\n",
    "    arr[~arr_mask] = np.inf\n",
    "    mins = arr.min(axis=1).astype(np.float32)\n",
    "    return mins\n",
    "\n",
    "\n",
    "def _pct_covered(times_min: np.ndarray, threshold_min: float, weights: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Weighted percent of population with times <= threshold.\n",
    "    \"\"\"\n",
    "    w = weights.astype(np.float64)\n",
    "    covered = (times_min <= threshold_min)\n",
    "    covered_pop = (w * covered).sum()\n",
    "    total_pop = w.sum()\n",
    "    return float(covered_pop / total_pop * 100.0) if total_pop > 0 else 0.0\n",
    "\n",
    "\n",
    "# ---- Build R (station → demand LSOA) ----\n",
    "R, R_mask = _build_sparse_matrix(\n",
    "    travel_df=travel,\n",
    "    row_codes=lsoa_index,          # rows: demand LSOAs\n",
    "    col_codes=station_lsoas,       # cols: station LSOAs\n",
    "    row_key=\"dest_lsoa\",           # time is origin→dest; we want station(origin)→demand(dest)\n",
    "    col_key=\"origin_lsoa\",\n",
    "    value_key=\"time_car_min\",\n",
    "    max_radius=MAX_RADIUS_MIN,\n",
    ")\n",
    "\n",
    "# ---- Build C (demand LSOA → acute) ----\n",
    "C, C_mask = _build_sparse_matrix(\n",
    "    travel_df=travel,\n",
    "    row_codes=lsoa_index,          # rows: demand LSOAs\n",
    "    col_codes=acute_lsoas,         # cols: acute LSOAs\n",
    "    row_key=\"origin_lsoa\",         # time is origin→dest; we want demand(origin)→acute(dest)\n",
    "    col_key=\"dest_lsoa\",\n",
    "    value_key=\"time_car_min\",\n",
    "    max_radius=MAX_RADIUS_MIN,\n",
    ")\n",
    "\n",
    "# ---- Basic integrity & density report ----\n",
    "def _report_matrix(name: str, mat: sparse.csr_matrix):\n",
    "    nnz = mat.nnz\n",
    "    m, n = mat.shape\n",
    "    density = nnz / (m * n) if (m > 0 and n > 0) else 0.0\n",
    "    print(f\"[OK] {name}: shape={mat.shape}, nnz={nnz:,}, density={density:.4f}\")\n",
    "\n",
    "_report_matrix(\"R (station→LSOA)\", R)\n",
    "_report_matrix(\"C (LSOA→acute)\", C)\n",
    "\n",
    "# ---- Cache matrices & metadata ----\n",
    "sparse.save_npz(MATRICES_DIR / \"R_response_csr.npz\", R)\n",
    "sparse.save_npz(MATRICES_DIR / \"R_response_mask_csr.npz\", R_mask)\n",
    "sparse.save_npz(MATRICES_DIR / \"C_convey_csr.npz\", C)\n",
    "sparse.save_npz(MATRICES_DIR / \"C_convey_mask_csr.npz\", C_mask)\n",
    "\n",
    "np.savez(\n",
    "    MATRICES_DIR / \"matrix_metadata.npz\",\n",
    "    lsoa_index=lsoa_index.to_numpy(),\n",
    "    station_lsoas=station_lsoas.to_numpy(),\n",
    "    acute_lsoas=acute_lsoas.to_numpy(),\n",
    "    response_thresholds=np.array(RESPONSE_THRESHOLDS, dtype=np.int32),\n",
    "    scene_to_ae_thresholds=np.array(SCENE_TO_AE_THRESHOLDS, dtype=np.int32),\n",
    "    blue_light_factors=np.array([BLUE_LIGHT_FACTOR_RESPONSE, BLUE_LIGHT_FACTOR_CONVEY], dtype=np.float32),\n",
    "    max_radius=np.array([np.nan if MAX_RADIUS_MIN is None else float(MAX_RADIUS_MIN)], dtype=np.float32),\n",
    ")\n",
    "print(f\"[OK] Cached matrices to: {MATRICES_DIR}\")\n",
    "\n",
    "# ---- Quick baseline sanity: nearest times & coverage (all stations / all acutes) ----\n",
    "t_resp_base = _rowwise_min_with_mask(R, R_mask) * np.float32(BLUE_LIGHT_FACTOR_RESPONSE)\n",
    "t_conv_base = _rowwise_min_with_mask(C, C_mask) * np.float32(BLUE_LIGHT_FACTOR_CONVEY)\n",
    "\n",
    "# Stats\n",
    "def _summ(name: str, arr: np.ndarray) -> str:\n",
    "    finite = np.isfinite(arr)\n",
    "    if not finite.any():\n",
    "        return f\"{name}: all inf\"\n",
    "    vals = arr[finite]\n",
    "    q = np.percentile(vals, [0, 25, 50, 90, 95, 100]).round(2)\n",
    "    return f\"{name}: min={q[0]}, p25={q[1]}, median={q[2]}, p90={q[3]}, p95={q[4]}, max={q[5]} (n={finite.sum()})\"\n",
    "\n",
    "print(_summ(\"t_resp (min to any station)\", t_resp_base))\n",
    "print(_summ(\"t_conv (min to any acute)\",   t_conv_base))\n",
    "\n",
    "# Weighted coverage (overall)\n",
    "pop_w = population.reindex(lsoa_index).to_numpy(dtype=np.float64)\n",
    "\n",
    "def _coverage_report(times: np.ndarray, thresholds: Sequence[int], label: str):\n",
    "    parts = []\n",
    "    for thr in thresholds:\n",
    "        pct = _pct_covered(times, thr, pop_w)\n",
    "        parts.append(f\"≤{thr} min: {pct:5.1f}%\")\n",
    "    print(f\"{label}: \" + \" | \".join(parts))\n",
    "\n",
    "# Response coverage at ARP-like thresholds (7, 15, 18, 40) for visibility\n",
    "_coverage_report(t_resp_base, RESPONSE_THRESHOLDS, \"Response coverage (all stations)\")\n",
    "\n",
    "# Conveyance coverage at scene→A&E bands (30, 45, 60)\n",
    "if C.shape[1] > 0:\n",
    "    _coverage_report(t_conv_base, SCENE_TO_AE_THRESHOLDS, \"Conveyance coverage (all acutes)\")\n",
    "else:\n",
    "    print(\"[WARN] No acute columns; conveyance coverage skipped.\")\n",
    "\n",
    "print(\"[OK] Step 2 complete — matrices built, cached, and baseline sanity computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0ab11a1-3ca6-4980-80cc-81a4182c56bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Baseline KPIs ==\n",
      "Response (ARP-like):\n",
      " threshold_min  pct_population\n",
      "             7           30.77\n",
      "            15           62.33\n",
      "            18           75.15\n",
      "            40           99.26\n",
      "\n",
      "Conveyance (scene→A&E bands):\n",
      " threshold_min  pct_population\n",
      "            30           51.77\n",
      "            45           69.35\n",
      "            60           79.03\n",
      "\n",
      "[OK] Exported:\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/tables/times_baseline.csv\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/tables/coverage_response_baseline.csv\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/tables/coverage_conveyance_baseline.csv\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/tables/coverage_by_lsoa_baseline.csv\n",
      "[OK] Step 3 complete — vectorised helpers run, KPIs exported, and per-LSOA flags ready for mapping.\n"
     ]
    }
   ],
   "source": [
    "# Step 3 — Vectorised helpers, baseline KPIs, and exports (no loops)\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Sequence, Dict, Tuple\n",
    "from scipy import sparse\n",
    "\n",
    "# --- Optional: set a visible on-scene buffer for end-to-end diagnostic (minutes)\n",
    "ON_SCENE_BUFFER_MIN: float = 0.0  # set >0 later if you want t_total\n",
    "\n",
    "# Column lookups (matrix columns are ordered by these indices)\n",
    "station_col_lookup: Dict[str, int] = {code: j for j, code in enumerate(station_lsoas)}\n",
    "acute_col_lookup: Dict[str, int]   = {code: j for j, code in enumerate(acute_lsoas)}\n",
    "\n",
    "def rowwise_min_with_mask(mat: sparse.csr_matrix, mask: sparse.csr_matrix) -> np.ndarray:\n",
    "    \"\"\"Row-wise minima treating 'no edge' as +inf (not 0).\"\"\"\n",
    "    if mat.shape[1] == 0:\n",
    "        return np.full(mat.shape[0], np.inf, dtype=np.float32)\n",
    "    arr = mat.toarray()\n",
    "    msk = mask.toarray().astype(bool)\n",
    "    arr[~msk] = np.inf\n",
    "    return arr.min(axis=1).astype(np.float32)\n",
    "\n",
    "def min_times_response(active_station_cols: np.ndarray | list[int]) -> pd.Series:\n",
    "    \"\"\"Nearest response time per LSOA (minutes), blue-light factor applied after min.\"\"\"\n",
    "    if R.shape[1] == 0 or len(active_station_cols) == 0:\n",
    "        mins = np.full(R.shape[0], np.inf, dtype=np.float32)\n",
    "    else:\n",
    "        sub = R[:, active_station_cols]\n",
    "        sub_mask = R_mask[:, active_station_cols]\n",
    "        mins = rowwise_min_with_mask(sub, sub_mask)\n",
    "    mins = mins * np.float32(BLUE_LIGHT_FACTOR_RESPONSE)\n",
    "    return pd.Series(mins, index=lsoa_index, name=\"t_resp_min\")\n",
    "\n",
    "def min_times_convey(active_acute_cols: np.ndarray | list[int]) -> pd.Series:\n",
    "    \"\"\"Nearest conveyance time per LSOA (minutes), blue-light factor applied after min.\"\"\"\n",
    "    if C.shape[1] == 0 or len(active_acute_cols) == 0:\n",
    "        mins = np.full(C.shape[0], np.inf, dtype=np.float32)\n",
    "    else:\n",
    "        sub = C[:, active_acute_cols]\n",
    "        sub_mask = C_mask[:, active_acute_cols]\n",
    "        mins = rowwise_min_with_mask(sub, sub_mask)\n",
    "    mins = mins * np.float32(BLUE_LIGHT_FACTOR_CONVEY)\n",
    "    return pd.Series(mins, index=lsoa_index, name=\"t_conv_min\")\n",
    "\n",
    "def coverage_table(times: pd.Series, thresholds: Sequence[int], weights: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Population-weighted coverage for multiple thresholds.\"\"\"\n",
    "    w = weights.reindex(times.index).astype(\"float64\")\n",
    "    total = float(w.sum())\n",
    "    out = []\n",
    "    a = times.to_numpy()\n",
    "    for thr in thresholds:\n",
    "        covered = (a <= thr)\n",
    "        pct = float((w.to_numpy() * covered).sum() / total * 100.0) if total > 0 else 0.0\n",
    "        out.append({\"threshold_min\": int(thr), \"pct_population\": round(pct, 2)})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def label_columns_for_export(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Make boolean coverage columns tidy (0/1 uint8) and keep friendly order.\"\"\"\n",
    "    bool_cols = [c for c in df.columns if c.startswith((\"resp_le_\", \"conv_le_\"))]\n",
    "    for c in bool_cols:\n",
    "        df[c] = df[c].astype(\"uint8\")\n",
    "    order = [\"t_resp_min\", \"t_conv_min\", \"t_total_min\"] + bool_cols\n",
    "    return df[[c for c in order if c in df.columns]]\n",
    "\n",
    "# --- Baseline: use all stations and all acutes ---\n",
    "baseline_station_cols = np.arange(R.shape[1], dtype=np.int32)\n",
    "baseline_acute_cols   = np.arange(C.shape[1], dtype=np.int32)\n",
    "\n",
    "t_resp = min_times_response(baseline_station_cols)\n",
    "t_conv = min_times_convey(baseline_acute_cols)\n",
    "\n",
    "# Optional end-to-end diagnostic\n",
    "t_total = (t_resp.to_numpy() + ON_SCENE_BUFFER_MIN + t_conv.to_numpy()).astype(np.float32)\n",
    "t_total = pd.Series(t_total, index=lsoa_index, name=\"t_total_min\")\n",
    "\n",
    "# --- Binary coverage flags for mapping/summary ---\n",
    "out_df = pd.DataFrame(index=lsoa_index)\n",
    "out_df[\"t_resp_min\"]  = t_resp\n",
    "out_df[\"t_conv_min\"]  = t_conv\n",
    "out_df[\"t_total_min\"] = t_total\n",
    "\n",
    "for thr in RESPONSE_THRESHOLDS:\n",
    "    out_df[f\"resp_le_{thr}\"] = (out_df[\"t_resp_min\"] <= thr)\n",
    "for thr in SCENE_TO_AE_THRESHOLDS:\n",
    "    out_df[f\"conv_le_{thr}\"] = (out_df[\"t_conv_min\"] <= thr)\n",
    "\n",
    "out_df = label_columns_for_export(out_df)\n",
    "\n",
    "# --- KPI tables (overall coverage) ---\n",
    "resp_kpis = coverage_table(t_resp, RESPONSE_THRESHOLDS, population)\n",
    "conv_kpis = coverage_table(t_conv, SCENE_TO_AE_THRESHOLDS, population)\n",
    "\n",
    "print(\"\\n== Baseline KPIs ==\")\n",
    "print(\"Response (ARP-like):\")\n",
    "print(resp_kpis.to_string(index=False))\n",
    "print(\"\\nConveyance (scene→A&E bands):\")\n",
    "print(conv_kpis.to_string(index=False))\n",
    "\n",
    "# --- Exports (tables/) ---\n",
    "times_path = TABLES_DIR / \"times_baseline.csv\"\n",
    "kpi_resp_path = TABLES_DIR / \"coverage_response_baseline.csv\"\n",
    "kpi_conv_path = TABLES_DIR / \"coverage_conveyance_baseline.csv\"\n",
    "by_lsoa_path = TABLES_DIR / \"coverage_by_lsoa_baseline.csv\"\n",
    "\n",
    "t_export = out_df.copy()\n",
    "t_export.insert(0, \"lsoa_code\", t_export.index)\n",
    "t_export.to_csv(times_path, index=False)\n",
    "resp_kpis.to_csv(kpi_resp_path, index=False)\n",
    "conv_kpis.to_csv(kpi_conv_path, index=False)\n",
    "\n",
    "# Coverage by LSOA (include population)\n",
    "by_lsoa = out_df.copy()\n",
    "by_lsoa.insert(0, \"lsoa_code\", by_lsoa.index)\n",
    "by_lsoa[\"population\"] = population.reindex(by_lsoa.index).astype(int).to_numpy()\n",
    "by_lsoa.to_csv(by_lsoa_path, index=False)\n",
    "\n",
    "print(\"\\n[OK] Exported:\")\n",
    "print(\" -\", times_path)\n",
    "print(\" -\", kpi_resp_path)\n",
    "print(\" -\", kpi_conv_path)\n",
    "print(\" -\", by_lsoa_path)\n",
    "print(\"[OK] Step 3 complete — vectorised helpers run, KPIs exported, and per-LSOA flags ready for mapping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "850ad64e-fe26-49f4-b675-d3876cb930b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Scenario exports written:\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/tables/times_baseline.csv\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/tables/scenario_kpis.csv\n",
      "[OK] Step 4 complete — scenario scaffold in place. Add station codes to test what-ifs instantly.\n"
     ]
    }
   ],
   "source": [
    "# Step 4 — Scenario scaffold (instant what-ifs by column subset; add stations later)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Scenario:\n",
    "    name: str\n",
    "    station_cols: np.ndarray  # indices into R's columns (0..R.shape[1]-1)\n",
    "    acute_cols:   np.ndarray  # indices into C's columns (0..C.shape[1]-1)\n",
    "\n",
    "def run_scenario(scn: Scenario) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Compute times and KPI tables for a scenario.\"\"\"\n",
    "    t_r = min_times_response(scn.station_cols)\n",
    "    t_c = min_times_convey(scn.acute_cols)\n",
    "    out = {\n",
    "        \"times\": pd.DataFrame(\n",
    "            {\"lsoa_code\": t_r.index, \"t_resp_min\": t_r.values, \"t_conv_min\": t_c.values}\n",
    "        )\n",
    "    }\n",
    "    out[\"resp_kpis\"] = coverage_table(t_r, RESPONSE_THRESHOLDS, population)\n",
    "    out[\"conv_kpis\"] = coverage_table(t_c, SCENE_TO_AE_THRESHOLDS, population)\n",
    "    return out\n",
    "\n",
    "# Baseline scenario = use all available columns as in Step 3\n",
    "SCENARIOS = [\n",
    "    Scenario(\"baseline\", station_cols=baseline_station_cols, acute_cols=baseline_acute_cols),\n",
    "    # Example “add one station” by code (uncomment and change code to a valid station LSOA):\n",
    "    # Scenario(\n",
    "    #     \"add_station_E01XXXXXX\",\n",
    "    #     station_cols=np.sort(np.unique(np.r_[baseline_station_cols, station_col_lookup[\"E01XXXXXX\"]])).astype(np.int32),\n",
    "    #     acute_cols=baseline_acute_cols,\n",
    "    # ),\n",
    "]\n",
    "\n",
    "# Run and export all scenarios\n",
    "rows = []\n",
    "for scn in SCENARIOS:\n",
    "    res = run_scenario(scn)\n",
    "    # Save per-scenario times\n",
    "    times_path = TABLES_DIR / f\"times_{scn.name}.csv\"\n",
    "    res[\"times\"].to_csv(times_path, index=False)\n",
    "    # Record KPIs in a flat table\n",
    "    for _, r in res[\"resp_kpis\"].iterrows():\n",
    "        rows.append({\"scenario\": scn.name, \"leg\": \"response\", \"threshold_min\": int(r[\"threshold_min\"]), \"pct_population\": float(r[\"pct_population\"])})\n",
    "    for _, r in res[\"conv_kpis\"].iterrows():\n",
    "        rows.append({\"scenario\": scn.name, \"leg\": \"conveyance\", \"threshold_min\": int(r[\"threshold_min\"]), \"pct_population\": float(r[\"pct_population\"])})\n",
    "\n",
    "scen_kpis = pd.DataFrame(rows).sort_values([\"scenario\", \"leg\", \"threshold_min\"])\n",
    "scen_path = TABLES_DIR / \"scenario_kpis.csv\"\n",
    "scen_kpis.to_csv(scen_path, index=False)\n",
    "\n",
    "print(\"\\n[OK] Scenario exports written:\")\n",
    "for scn in SCENARIOS:\n",
    "    print(\" -\", TABLES_DIR / f\"times_{scn.name}.csv\")\n",
    "print(\" -\", scen_path)\n",
    "print(\"[OK] Step 4 complete — scenario scaffold in place. Add station codes to test what-ifs instantly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7486345-fa4a-4a8d-bb0e-a745479dff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] No non-baseline scenarios defined. Wrote an empty scenario_delta_summary.csv.\n",
      "[OK] Step 5 complete — per-scenario deltas, KPI shifts, and QA checks exported.\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 — Scenario diffs, exports, and QA (robust to no scenarios)\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Sequence\n",
    "\n",
    "# --- Helper: build a scenario by adding station LSOA codes (validates codes) ---\n",
    "def make_add_station_scenario(name: str, add_station_codes: Sequence[str]) -> Scenario:\n",
    "    missing = [c for c in add_station_codes if c not in station_col_lookup]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Unknown station LSOA codes: {missing}\")\n",
    "    add_cols = np.array([station_col_lookup[c] for c in add_station_codes], dtype=np.int32)\n",
    "    new_cols = np.sort(np.unique(np.r_[baseline_station_cols, add_cols])).astype(np.int32)\n",
    "    return Scenario(name=name, station_cols=new_cols, acute_cols=baseline_acute_cols)\n",
    "\n",
    "# Example usage (uncomment and replace with real code):\n",
    "# SCENARIOS.append(make_add_station_scenario(\"add_station_E01XXXXXX\", [\"E01XXXXXX\"]))\n",
    "\n",
    "def _scenario_times(station_cols: np.ndarray, acute_cols: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"Return per-LSOA times for a scenario (response, convey, total).\"\"\"\n",
    "    t_r = min_times_response(station_cols)\n",
    "    t_c = min_times_convey(acute_cols)\n",
    "    t_total = (t_r.to_numpy() + ON_SCENE_BUFFER_MIN + t_c.to_numpy()).astype(np.float32)\n",
    "    return pd.DataFrame(\n",
    "        {\"lsoa_code\": lsoa_index, \"t_resp_min\": t_r.values, \"t_conv_min\": t_c.values, \"t_total_min\": t_total}\n",
    "    )\n",
    "\n",
    "def _coverage_kpis(times_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"KPI tables for response and conveyance.\"\"\"\n",
    "    resp_kpis = coverage_table(times_df.set_index(\"lsoa_code\")[\"t_resp_min\"], RESPONSE_THRESHOLDS, population)\n",
    "    conv_kpis = coverage_table(times_df.set_index(\"lsoa_code\")[\"t_conv_min\"], SCENE_TO_AE_THRESHOLDS, population)\n",
    "    return resp_kpis, conv_kpis\n",
    "\n",
    "# Baseline artefacts (compute once)\n",
    "baseline_times = _scenario_times(baseline_station_cols, baseline_acute_cols)\n",
    "base_resp_kpi, base_conv_kpi = _coverage_kpis(baseline_times)\n",
    "\n",
    "# Filter to non-baseline scenarios\n",
    "scenarios_to_run = [s for s in SCENARIOS if s.name != \"baseline\"]\n",
    "\n",
    "rows_summary: list[Dict] = []\n",
    "if len(scenarios_to_run) == 0:\n",
    "    # Write an empty summary with headers so downstream steps don't break\n",
    "    summary_df = pd.DataFrame(\n",
    "        columns=[\"scenario\", \"w_mean_resp_minutes_saved\", \"w_mean_total_minutes_saved\", \"n_worsened_resp\"]\n",
    "    )\n",
    "    summary_path = TABLES_DIR / \"scenario_delta_summary.csv\"\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(\"[WARN] No non-baseline scenarios defined. Wrote an empty scenario_delta_summary.csv.\")\n",
    "else:\n",
    "    for scn in scenarios_to_run:\n",
    "        scn_times = _scenario_times(scn.station_cols, scn.acute_cols)\n",
    "\n",
    "        # --- LSOA-level deltas (positive = minutes saved) ---\n",
    "        merged = baseline_times.merge(scn_times, on=\"lsoa_code\", suffixes=(\"_base\", \"_scn\"))\n",
    "        merged[\"d_resp_min\"]  = merged[\"t_resp_min_base\"]  - merged[\"t_resp_min_scn\"]\n",
    "        merged[\"d_conv_min\"]  = merged[\"t_conv_min_base\"]  - merged[\"t_conv_min_scn\"]\n",
    "        merged[\"d_total_min\"] = merged[\"t_total_min_base\"] - merged[\"t_total_min_scn\"]\n",
    "\n",
    "        # Monotonicity QA — response shouldn't worsen when adding stations\n",
    "        worsened = int((merged[\"d_resp_min\"] < -1e-6).sum())\n",
    "        if worsened:\n",
    "            print(f\"[WARN] {scn.name}: {worsened} LSOAs have worse response times than baseline.\")\n",
    "\n",
    "        # Top improvements (absolute minutes saved) — export a ranked view\n",
    "        topN = (\n",
    "            merged[[\"lsoa_code\", \"d_resp_min\", \"d_conv_min\", \"d_total_min\"]]\n",
    "            .sort_values(\"d_resp_min\", ascending=False)\n",
    "            .head(25)\n",
    "            .copy()\n",
    "        )\n",
    "\n",
    "        # KPI deltas\n",
    "        scn_resp_kpi, scn_conv_kpi = _coverage_kpis(scn_times)\n",
    "\n",
    "        resp_delta = scn_resp_kpi.merge(base_resp_kpi, on=\"threshold_min\", suffixes=(\"_scn\", \"_base\"))\n",
    "        resp_delta[\"delta_pp\"] = resp_delta[\"pct_population_scn\"] - resp_delta[\"pct_population_base\"]\n",
    "\n",
    "        conv_delta = scn_conv_kpi.merge(base_conv_kpi, on=\"threshold_min\", suffixes=(\"_scn\", \"_base\"))\n",
    "        conv_delta[\"delta_pp\"] = conv_delta[\"pct_population_scn\"] - conv_delta[\"pct_population_base\"]\n",
    "\n",
    "        # Population-weighted average minutes saved\n",
    "        pop = population.reindex(merged[\"lsoa_code\"]).to_numpy(dtype=float)\n",
    "        tot_pop = pop.sum() if pop.sum() > 0 else 1.0\n",
    "        w_mean_resp_save  = float((pop * merged[\"d_resp_min\"].to_numpy()).sum() / tot_pop)\n",
    "        w_mean_total_save = float((pop * merged[\"d_total_min\"].to_numpy()).sum() / tot_pop)\n",
    "\n",
    "        rows_summary.append(\n",
    "            {\n",
    "                \"scenario\": scn.name,\n",
    "                \"w_mean_resp_minutes_saved\": round(w_mean_resp_save, 3),\n",
    "                \"w_mean_total_minutes_saved\": round(w_mean_total_save, 3),\n",
    "                \"n_worsened_resp\": worsened,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # --- Exports per scenario ---\n",
    "        per_lsoa_path = TABLES_DIR / f\"delta_by_lsoa_{scn.name}.csv\"\n",
    "        topN_path     = TABLES_DIR / f\"top25_improvements_{scn.name}.csv\"\n",
    "        resp_kpi_path = TABLES_DIR / f\"delta_kpi_response_{scn.name}.csv\"\n",
    "        conv_kpi_path = TABLES_DIR / f\"delta_kpi_convey_{scn.name}.csv\"\n",
    "\n",
    "        merged.to_csv(per_lsoa_path, index=False)\n",
    "        topN.to_csv(topN_path, index=False)\n",
    "        resp_delta.to_csv(resp_kpi_path, index=False)\n",
    "        conv_delta.to_csv(conv_kpi_path, index=False)\n",
    "\n",
    "        print(f\"[OK] {scn.name}: wrote\")\n",
    "        print(\"   -\", per_lsoa_path.name)\n",
    "        print(\"   -\", topN_path.name)\n",
    "        print(\"   -\", resp_kpi_path.name)\n",
    "        print(\"   -\", conv_kpi_path.name)\n",
    "\n",
    "    # Consolidated scenario summary\n",
    "    summary_df = pd.DataFrame(rows_summary).sort_values(\"scenario\")\n",
    "    summary_path = TABLES_DIR / \"scenario_delta_summary.csv\"\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(\"[OK] Scenario delta summary →\", summary_path)\n",
    "\n",
    "print(\"[OK] Step 5 complete — per-scenario deltas, KPI shifts, and QA checks exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b38db28-f9a3-4082-a5ee-50138f08aa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Step 6 complete — maps written:\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_response_le_7min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_response_le_15min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_response_le_18min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_response_le_40min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_conveyance_le_30min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_conveyance_le_45min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_conveyance_le_60min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_t_resp_min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_t_conv_min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_t_total_min.png\n"
     ]
    }
   ],
   "source": [
    "# STEP 6 — Maps (binary coverage choropleths + continuous time layers)\n",
    "\n",
    "from __future__ import annotations\n",
    "import contextlib\n",
    "\n",
    "# Requirements from previous steps:\n",
    "# - lsoa_g  : GeoDataFrame aligned to lsoa_index (EPSG:27700)\n",
    "# - out_df  : DataFrame with columns t_resp_min, t_conv_min, (optional) t_total_min,\n",
    "#             and boolean flags resp_le_{thr}, conv_le_{thr}\n",
    "# - station_lsoas, acute_lsoas : Index of LSOA codes\n",
    "# - MAPS_DIR : output directory (Path)\n",
    "# - RESPONSE_THRESHOLDS, SCENE_TO_AE_THRESHOLDS\n",
    "\n",
    "# ---------- prepare mapping frame ----------\n",
    "# Join mapping geometry with times/flags\n",
    "gmap = lsoa_g.join(out_df, how=\"left\")\n",
    "\n",
    "# Derive station/acute points by taking LSOA centroids (no site coordinates assumed)\n",
    "# (EPSG:27700 is projected, so centroid is safe)\n",
    "centroids = gmap.geometry.centroid  # noqa: SHP.W001 (projected CRS)\n",
    "station_pts = gpd.GeoDataFrame(\n",
    "    {\"lsoa_code\": station_lsoas}, geometry=centroids.reindex(station_lsoas), crs=gmap.crs\n",
    ")\n",
    "acute_pts = gpd.GeoDataFrame(\n",
    "    {\"lsoa_code\": acute_lsoas}, geometry=centroids.reindex(acute_lsoas), crs=gmap.crs\n",
    ")\n",
    "\n",
    "# ---------- styling helpers ----------\n",
    "COVERED_COLOUR = \"#2ca25f\"\n",
    "UNCOVERED_COLOUR = \"#de2d26\"\n",
    "BORDER_COLOUR = \"#ffffff\"\n",
    "BG_COLOUR = \"#f7f7f7\"\n",
    "PTS_STATION_COLOUR = \"#1f78b4\"\n",
    "PTS_ACUTE_COLOUR = \"#6a3d9a\"\n",
    "\n",
    "def _legend_binary(ax, covered_label: str = \"Covered\", uncovered_label: str = \"Not covered\"):\n",
    "    patches = [\n",
    "        Patch(facecolor=COVERED_COLOUR, edgecolor=BORDER_COLOUR, label=covered_label),\n",
    "        Patch(facecolor=UNCOVERED_COLOUR, edgecolor=BORDER_COLOUR, label=uncovered_label),\n",
    "        Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=PTS_STATION_COLOUR, markeredgecolor=\"none\",\n",
    "               markersize=8, label=\"Station (centroid)\"),\n",
    "        Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=PTS_ACUTE_COLOUR, markeredgecolor=\"none\",\n",
    "               markersize=8, label=\"Acute (centroid)\"),\n",
    "    ]\n",
    "    ax.legend(handles=patches, loc=\"lower left\", frameon=True, framealpha=0.9)\n",
    "\n",
    "def _plot_binary(layer_col: str, title: str, outfile: Path):\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 9), dpi=150, facecolor=\"white\")\n",
    "    ax.set_facecolor(BG_COLOUR)\n",
    "\n",
    "    # Fallback if column missing\n",
    "    if layer_col not in gmap.columns:\n",
    "        ax.text(0.5, 0.5, f\"Column '{layer_col}' not found.\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "        plt.savefig(outfile, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    covered = gmap[gmap[layer_col] == True]   # noqa: E712\n",
    "    not_covered = gmap[gmap[layer_col] != True]\n",
    "\n",
    "    # Draw polygons\n",
    "    with contextlib.suppress(Exception):\n",
    "        not_covered.plot(ax=ax, color=UNCOVERED_COLOUR, edgecolor=BORDER_COLOUR, linewidth=0.2)\n",
    "        covered.plot(ax=ax, color=COVERED_COLOUR, edgecolor=BORDER_COLOUR, linewidth=0.2)\n",
    "\n",
    "    # Overlays\n",
    "    if not station_pts.empty:\n",
    "        station_pts.plot(ax=ax, markersize=10, color=PTS_STATION_COLOUR, alpha=0.9)\n",
    "    if not acute_pts.empty:\n",
    "        acute_pts.plot(ax=ax, markersize=10, color=PTS_ACUTE_COLOUR, alpha=0.9)\n",
    "\n",
    "    ax.set_title(title, fontsize=13, pad=10)\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def _plot_continuous(value_col: str, title: str, outfile: Path, vmin: float | None = None, vmax: float | None = None):\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 9), dpi=150, facecolor=\"white\")\n",
    "    ax.set_facecolor(BG_COLOUR)\n",
    "\n",
    "    if value_col not in gmap.columns:\n",
    "        ax.text(0.5, 0.5, f\"Column '{value_col}' not found.\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "        plt.savefig(outfile, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    # Clip to finite range; auto vmin/vmax if not provided\n",
    "    data = gmap[value_col].replace([np.inf, -np.inf], np.nan)\n",
    "    if vmin is None:\n",
    "        vmin = float(np.nanpercentile(data, 2)) if np.isfinite(data).any() else 0.0\n",
    "    if vmax is None:\n",
    "        vmax = float(np.nanpercentile(data, 98)) if np.isfinite(data).any() else 1.0\n",
    "    vmin, vmax = (min(vmin, vmax), max(vmin, vmax))\n",
    "\n",
    "    with contextlib.suppress(Exception):\n",
    "        gmap.plot(\n",
    "            column=value_col, ax=ax, cmap=\"viridis\", vmin=vmin, vmax=vmax,\n",
    "            edgecolor=BORDER_COLOUR, linewidth=0.2, legend=True,\n",
    "            legend_kwds={\"label\": \"Minutes\", \"shrink\": 0.6},\n",
    "        )\n",
    "\n",
    "    if not station_pts.empty:\n",
    "        station_pts.plot(ax=ax, markersize=10, color=PTS_STATION_COLOUR, alpha=0.9)\n",
    "    if not acute_pts.empty:\n",
    "        acute_pts.plot(ax=ax, markersize=10, color=PTS_ACUTE_COLOUR, alpha=0.9)\n",
    "\n",
    "    ax.set_title(title, fontsize=13, pad=10)\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------- generate & save maps ----------\n",
    "written = []\n",
    "\n",
    "# Binary coverage — response thresholds\n",
    "for thr in RESPONSE_THRESHOLDS:\n",
    "    col = f\"resp_le_{thr}\"\n",
    "    title = f\"Response coverage ≤{thr} min (ARP-aligned)\"\n",
    "    out = MAPS_DIR / f\"map_response_le_{thr}min.png\"\n",
    "    _plot_binary(col, title, out)\n",
    "    written.append(out)\n",
    "\n",
    "# Binary coverage — conveyance thresholds\n",
    "for thr in SCENE_TO_AE_THRESHOLDS:\n",
    "    col = f\"conv_le_{thr}\"\n",
    "    title = f\"Conveyance coverage ≤{thr} min (scene→A&E)\"\n",
    "    out = MAPS_DIR / f\"map_conveyance_le_{thr}min.png\"\n",
    "    _plot_binary(col, title, out)\n",
    "    written.append(out)\n",
    "\n",
    "# Continuous time surfaces\n",
    "_plot_continuous(\"t_resp_min\",  \"Nearest response time (min)\", MAPS_DIR / \"map_t_resp_min.png\")\n",
    "written.append(MAPS_DIR / \"map_t_resp_min.png\")\n",
    "\n",
    "_plot_continuous(\"t_conv_min\",  \"Nearest conveyance time (min)\", MAPS_DIR / \"map_t_conv_min.png\")\n",
    "written.append(MAPS_DIR / \"map_t_conv_min.png\")\n",
    "\n",
    "if \"t_total_min\" in gmap.columns:\n",
    "    _plot_continuous(\"t_total_min\", \"End-to-end time (resp + scene + convey)\", MAPS_DIR / \"map_t_total_min.png\")\n",
    "    written.append(MAPS_DIR / \"map_t_total_min.png\")\n",
    "\n",
    "print(\"[OK] Step 6 complete — maps written:\")\n",
    "for p in written:\n",
    "    print(\" -\", p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
