{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de0efb2-eba7-4d39-afa8-6bc28b724c46",
   "metadata": {},
   "source": [
    "### 02b_matrix_coverage_fast_ops — end-to-end, sparse-matrix engine \n",
    "\n",
    "**Notebook purpose (plain language)**  \n",
    "This notebook swaps the *engine* that finds minimum times in `02a_coverage`.  \n",
    "Instead of pandas group-bys on the long travel table, we build two *sparse* matrices and use fast, vectorised reductions. All inputs, thresholds, blue-light factors, KPIs, and maps stay the same—only the internals get faster and more scalable, enabling instant “what-if” scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "#### What this notebook does\n",
    "\n",
    "- **Builds matrices (reshape only, no routing):**  \n",
    "  - **R** (response): rows = demand LSOAs, cols = station LSOAs, values = minutes station→LSOA.  \n",
    "  - **C** (conveyance): rows = demand LSOAs, cols = acute LSOAs, values = minutes LSOA→acute.\n",
    "- **Computes nearest times (vectorised, no loops):**  \n",
    "  - `t_resp = rowwise_min(R[:, active_stations])`  \n",
    "  - `t_conv = rowwise_min(C[:, active_acutes])`\n",
    "- **Applies business rules:** blue-light factors applied *after* minima (per-leg), thresholds as in 02a.\n",
    "- **Outputs unchanged:** coverage KPIs (% pop within 7/15 and 18/40), binary coverage columns, maps.\n",
    "- **Adds optional diagnostic:** `t_total = t_resp + on_scene_buffer + t_conv` (three-leg view).\n",
    "- **Enables scenarios:** select different station sets by column subset—no re-grouping or re-reading.\n",
    "\n",
    "---\n",
    "\n",
    "#### Inputs (same as 02a)\n",
    "\n",
    "- LSOA universe (`lsoa_index`), centroids, populations (+ optional IMD / rural-urban).  \n",
    "- Station & acute site files resolved to LSOA codes.  \n",
    "- Long-form travel-time table already in the repo (response & conveyance legs).  \n",
    "- Thresholds & blue-light factors (ARP, handover, conveyance) defined up front.\n",
    "\n",
    "---\n",
    "\n",
    "#### Outputs\n",
    "\n",
    "- KPIs for response & conveyance at configured thresholds (overall and, optionally, by IMD/rural-urban).  \n",
    "- Binary coverage columns per threshold for mapping.  \n",
    "- (Optional) End-to-end time columns for transparency in pathway discussions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Performance & storage\n",
    "\n",
    "- **Sparse CSR** matrices for R and C; optional “has-edge” masks to distinguish true zeros from missing pairs.  \n",
    "- **Radius thinning** (e.g., drop times > 60 min) to shrink matrices without losing feasible options.  \n",
    "- **Caching:** save matrices + ordered labels to `data/.../matrices/*.npz` for instant reloads.\n",
    "\n",
    "---\n",
    "\n",
    "#### Scenario selector\n",
    "\n",
    "Define scenarios as sets of active station/acute columns (e.g., `baseline`, `baseline + Site X`).  \n",
    "Switching scenario = re-taking per-row minima → near-instant “what-if” diffs and coverage deltas.\n",
    "\n",
    "---\n",
    "\n",
    "#### Validation (first run)\n",
    "\n",
    "- **Parity check vs 02a:** times and KPIs should match within tight tolerance on the Cornwall slice.  \n",
    "- After validation, retire the legacy `min_time_from_any_origin` calls in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "#### Notes & cautions\n",
    "\n",
    "- Any change to travel-time inputs **invalidates caches** → rebuild matrices.  \n",
    "- LSOAs with no reachable station/acute remain at **∞** and are reported explicitly.  \n",
    "- Keep LSOA codes categorical and ordering stable to avoid misalignment bugs.\n",
    "\n",
    "---\n",
    "\n",
    "#### Quick explainer (02a → 02b)\n",
    "\n",
    "| Area / Step | 02a does now | 02b change | Benefit |\n",
    "|---|---|---|---|\n",
    "| Min times | pandas group-by on long table | rowwise min on sparse matrices | Much faster; scalable; no loops |\n",
    "| Scenarios | re-filter + re-group | column subset on R/C | Instant “what-if” |\n",
    "| Blue-light | applied in KPIs | apply after minima per leg | Correct nearest selection |\n",
    "| Outputs | KPIs, maps | same | No UX change |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf5fab5-3cd0-48be-8bef-1729b3a9364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Imports & file paths\n",
    "\n",
    "from pathlib import Path\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project root for this ICB slice\n",
    "DATA_ROOT = Path(\n",
    "    \"/Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/\"\n",
    "    \"GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level\"\n",
    ")\n",
    "\n",
    "# Inputs\n",
    "LOOKUP_CSV   = DATA_ROOT / \"cornwall_icb_lsoa_lookup.csv\"\n",
    "AGE_GPKG     = DATA_ROOT / \"demographics_age_continuous_icb.gpkg\"\n",
    "AGE_LAYER    = \"LSOA_continuous_age_icb\"\n",
    "TRAVEL_CSV   = DATA_ROOT / \"travel_matrix_lsoa_icb.csv\"\n",
    "STATIONS_CSV = DATA_ROOT / \"ambulance_stations_icb.csv\"\n",
    "ACUTE_CSV    = DATA_ROOT / \"acute_hospitals_icb.csv\"   # optional overlay\n",
    "\n",
    "# Outputs\n",
    "MATRICES_DIR = DATA_ROOT / \"matrices\"\n",
    "MAPS_DIR     = DATA_ROOT / \"maps\"\n",
    "TABLES_DIR   = DATA_ROOT / \"tables\"\n",
    "for d in (MATRICES_DIR, MAPS_DIR, TABLES_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Display prefs\n",
    "pd.set_option(\"display.width\", 120)\n",
    "pd.set_option(\"display.max_columns\", 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288fb434-a7f5-481c-9803-a54e43fe842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters: ARP thresholds, handover bands, blue-light factors\n",
    "\n",
    "# ARP response standards (minutes)\n",
    "RESP = {\n",
    "    \"cat1\": {\"mean\": 7,  \"p90\": 15},\n",
    "    \"cat2\": {\"mean\": 18, \"p90\": 40},\n",
    "    # optional completeness\n",
    "    \"cat3\": {\"p90\": 120},\n",
    "    \"cat4\": {\"p90\": 180},\n",
    "}\n",
    "\n",
    "# Convenience tuple for KPI lookups in this notebook\n",
    "RESPONSE_THRESHOLDS = (\n",
    "    RESP[\"cat1\"][\"mean\"], RESP[\"cat1\"][\"p90\"],\n",
    "    RESP[\"cat2\"][\"mean\"], RESP[\"cat2\"][\"p90\"]\n",
    ")  # -> (7, 15, 18, 40)\n",
    "\n",
    "# Hospital handover/turnaround (NOT scene→A&E drive time)\n",
    "HANDOVER = {\"target\": 15, \"breach\": 30, \"severe\": 60}\n",
    "HANDOVER_THRESHOLDS = (HANDOVER[\"target\"], HANDOVER[\"breach\"], HANDOVER[\"severe\"])\n",
    "\n",
    "# Scene→A&E conveyance bands (geographic potential; no national target)\n",
    "SCENE_TO_AE_THRESHOLDS = (30, 45, 60)\n",
    "\n",
    "# Optional blue-light multipliers (applied AFTER minima per leg)\n",
    "BLUE_LIGHT_FACTOR_RESPONSE = 1.0\n",
    "BLUE_LIGHT_FACTOR_CONVEY   = 1.0\n",
    "\n",
    "# End-to-end diagnostic buffer (minutes) for t_total (set >0 if you want to show it)\n",
    "ON_SCENE_BUFFER_MIN = 0.0\n",
    "\n",
    "# CRS for mapping\n",
    "TARGET_CRS = \"EPSG:27700\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0788e306-0c2f-4e34-a55d-9882bea8c03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Universe: 336 LSOAs\n",
      "[OK] Population sum=575,628; CRS=EPSG:27700\n",
      "[WARN] Floored 41 off-diagonal zero-minute rows to 0.5 min.\n",
      "[OK] Travel rows=112,560; origins=336; dests=336\n",
      "[OK] Stations=14 LSOAs; Acutes=3 LSOAs\n",
      "\n",
      "== STEP 1 SUMMARY ==\n",
      "n_lsoas               336\n",
      "population_sum     575628\n",
      "travel_rows        112560\n",
      "unique_origins        336\n",
      "unique_dests          336\n",
      "n_station_lsoas        14\n",
      "n_acute_lsoas           3\n",
      "[OK] Step 1 complete — data aligned and cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Step 1 — Load & align core data\n",
    "# What this does:\n",
    "# - Loads the LSOA universe, population & geometry, long travel table, and site lists.\n",
    "# - Cleans obvious time issues (negative, off-diagonal zeros).\n",
    "# - Produces aligned indices, population vector, and site code arrays for later steps.\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Iterable, Dict\n",
    "import re\n",
    "\n",
    "# ---- small helpers ----\n",
    "def _ok(msg: str) -> None:   print(f\"[OK] {msg}\")\n",
    "def _warn(msg: str) -> None: print(f\"[WARN] {msg}\")\n",
    "def _fail(msg: str) -> None: raise AssertionError(msg)\n",
    "def _expect_columns(df: pd.DataFrame, cols: Iterable[str], label: str) -> None:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing: _fail(f\"{label}: missing columns {missing}\")\n",
    "\n",
    "# ---- 1.1 LSOA universe ----\n",
    "lookup = pd.read_csv(LOOKUP_CSV, dtype={\"lsoa_code\": \"string\"})\n",
    "_expect_columns(lookup, [\"lsoa_code\"], \"LSOA lookup\")\n",
    "lookup = lookup.drop_duplicates(subset=[\"lsoa_code\"]).copy()\n",
    "lsoa_index = pd.Index(lookup[\"lsoa_code\"].astype(\"string\"), name=\"lsoa_code\")\n",
    "if lsoa_index.empty or not lsoa_index.is_unique:\n",
    "    _fail(\"LSOA lookup must provide a non-empty, unique list of LSOA codes.\")\n",
    "_ok(f\"Universe: {len(lsoa_index):,} LSOAs\")\n",
    "\n",
    "# ---- 1.2 Population & geometry ----\n",
    "lsoa_g = gpd.read_file(AGE_GPKG, layer=AGE_LAYER)\n",
    "_expect_columns(lsoa_g, [\"lsoa_code\", \"geometry\"], \"Age GPKG\")\n",
    "lsoa_g[\"lsoa_code\"] = lsoa_g[\"lsoa_code\"].astype(\"string\")\n",
    "\n",
    "# Prefer 'population_total' if present, else sum continuous age columns\n",
    "pop_col = next((c for c in (\"population_total\", \"population\") if c in lsoa_g.columns), None)\n",
    "if pop_col:\n",
    "    population = lsoa_g.set_index(\"lsoa_code\")[pop_col].astype(\"float64\")\n",
    "else:\n",
    "    age_cols = []\n",
    "    for c in lsoa_g.columns:\n",
    "        if c in (\"lsoa_code\", \"geometry\"): continue\n",
    "        if (re.fullmatch(r\"\\d{1,3}\\+?\", str(c)) or str(c).startswith(\"age_\")) and np.issubdtype(lsoa_g[c].dtype, np.number):\n",
    "            age_cols.append(c)\n",
    "    if not age_cols:\n",
    "        _fail(\"No 'population_total' nor numeric age columns found.\")\n",
    "    population = lsoa_g.set_index(\"lsoa_code\")[age_cols].sum(axis=1).astype(\"float64\")\n",
    "\n",
    "# Align population & geometry to universe\n",
    "population = population.reindex(lsoa_index).fillna(0.0)\n",
    "lsoa_g = (\n",
    "    lsoa_g[[\"lsoa_code\", \"geometry\"]]\n",
    "    .drop_duplicates(\"lsoa_code\")\n",
    "    .set_index(\"lsoa_code\")\n",
    "    .reindex(lsoa_index)\n",
    ")\n",
    "lsoa_g = gpd.GeoDataFrame(lsoa_g, geometry=\"geometry\", crs=lsoa_g.crs)\n",
    "_ok(f\"Population sum={int(population.sum()):,}; CRS={lsoa_g.crs}\")\n",
    "\n",
    "# ---- 1.3 Travel table (load → normalise → clean) ----\n",
    "travel = pd.read_csv(\n",
    "    TRAVEL_CSV,\n",
    "    dtype={\"origin_lsoa\": \"string\", \"dest_lsoa\": \"string\"},\n",
    ")\n",
    "# Normalise minutes column to 'time_car_min'\n",
    "time_col = next((c for c in (\"time_car_min\", \"time_min\", \"minutes\", \"drive_min\", \"t_min\") if c in travel.columns), None)\n",
    "if time_col is None:\n",
    "    _fail(\"Travel CSV must include a minutes column (e.g., 'time_car_min').\")\n",
    "travel = travel.rename(columns={time_col: \"time_car_min\"})\n",
    "_expect_columns(travel, [\"origin_lsoa\", \"dest_lsoa\", \"time_car_min\"], \"Travel CSV\")\n",
    "\n",
    "# Types & NA\n",
    "travel[\"origin_lsoa\"] = travel[\"origin_lsoa\"].astype(\"string\")\n",
    "travel[\"dest_lsoa\"]   = travel[\"dest_lsoa\"].astype(\"string\")\n",
    "travel[\"time_car_min\"] = pd.to_numeric(travel[\"time_car_min\"], errors=\"coerce\").astype(\"float32\")\n",
    "travel = travel.dropna(subset=[\"origin_lsoa\", \"dest_lsoa\", \"time_car_min\"]).copy()\n",
    "\n",
    "# Keep only rows inside universe\n",
    "in_uni = travel[\"origin_lsoa\"].isin(lsoa_index) & travel[\"dest_lsoa\"].isin(lsoa_index)\n",
    "dropped = int((~in_uni).sum())\n",
    "if dropped: _warn(f\"Dropping {dropped:,} travel rows outside universe.\")\n",
    "travel = travel.loc[in_uni].copy()\n",
    "\n",
    "# Clean non-positive and huge times (policy: floor off-diagonal zeros to 0.5; drop negatives)\n",
    "is_diag = travel[\"origin_lsoa\"] == travel[\"dest_lsoa\"]\n",
    "offdiag_zero = (travel[\"time_car_min\"] <= 0) & (~is_diag)\n",
    "neg_rows = int((travel[\"time_car_min\"] < 0).sum())\n",
    "if neg_rows:\n",
    "    travel = travel.loc[travel[\"time_car_min\"] >= 0].copy()\n",
    "    _warn(f\"Dropped {neg_rows:,} negative-minute rows.\")\n",
    "floored = int(offdiag_zero.sum())\n",
    "if floored:\n",
    "    travel.loc[offdiag_zero, \"time_car_min\"] = np.float32(0.5)\n",
    "    _warn(f\"Floored {floored:,} off-diagonal zero-minute rows to 0.5 min.\")\n",
    "_ok(f\"Travel rows={len(travel):,}; origins={travel['origin_lsoa'].nunique():,}; dests={travel['dest_lsoa'].nunique():,}\")\n",
    "\n",
    "# ---- 1.4 Sites (stations & acutes → LSOA codes) ----\n",
    "def _load_site_lsoas(csv_path: Path, label: str) -> pd.Index:\n",
    "    if not csv_path.exists():\n",
    "        if label.lower().startswith(\"acute\"):\n",
    "            _warn(\"Acute CSV not found; conveyance leg optional.\")\n",
    "            return pd.Index([], dtype=\"string\", name=\"lsoa_code\")\n",
    "        _fail(f\"{label} CSV not found: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    code_col = next((c for c in (\"lsoa_code\", \"lsoa21cd\") if c in df.columns), None)\n",
    "    if code_col is None: _fail(f\"{label}: expected 'lsoa_code' or 'lsoa21cd'.\")\n",
    "    codes = pd.Index(df[code_col].astype(\"string\"), name=\"lsoa_code\").dropna().drop_duplicates()\n",
    "    codes = codes[codes.isin(lsoa_index)]\n",
    "    if codes.empty: _warn(f\"{label}: no valid LSOAs after filtering to universe.\")\n",
    "    return codes\n",
    "\n",
    "station_lsoas = _load_site_lsoas(STATIONS_CSV, \"Ambulance stations\")\n",
    "acute_lsoas   = _load_site_lsoas(ACUTE_CSV,    \"Acute hospitals\")\n",
    "_ok(f\"Stations={len(station_lsoas)} LSOAs; Acutes={len(acute_lsoas)} LSOAs\")\n",
    "\n",
    "# ---- 1.5 Integer maps for later matrix ops ----\n",
    "lsoa_to_idx: Dict[str, int] = {code: i for i, code in enumerate(lsoa_index)}\n",
    "idx_to_lsoa = lsoa_index.to_numpy()\n",
    "station_idx = np.array([lsoa_to_idx[c] for c in station_lsoas], dtype=np.int32) if len(station_lsoas) else np.array([], dtype=np.int32)\n",
    "acute_idx   = np.array([lsoa_to_idx[c] for c in acute_lsoas],   dtype=np.int32) if len(acute_lsoas)   else np.array([], dtype=np.int32)\n",
    "\n",
    "# ---- 1.6 Quick readout ----\n",
    "print(\"\\n== STEP 1 SUMMARY ==\")\n",
    "print(pd.Series({\n",
    "    \"n_lsoas\": len(lsoa_index),\n",
    "    \"population_sum\": int(population.sum()),\n",
    "    \"travel_rows\": len(travel),\n",
    "    \"unique_origins\": travel[\"origin_lsoa\"].nunique(),\n",
    "    \"unique_dests\": travel[\"dest_lsoa\"].nunique(),\n",
    "    \"n_station_lsoas\": len(station_lsoas),\n",
    "    \"n_acute_lsoas\": len(acute_lsoas),\n",
    "}).to_string())\n",
    "_ok(\"Step 1 complete — data aligned and cleaned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02488d7-9499-4c9a-a1d5-fe98e8893d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] R (station→LSOA): shape=(336, 14), nnz=4,690, density=0.9970\n",
      "[OK] C (LSOA→acute): shape=(336, 3), nnz=1,005, density=0.9970\n",
      "[OK] Cached matrices & metadata → /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/matrices\n"
     ]
    }
   ],
   "source": [
    "# Step 2 — Build & cache sparse matrices (R: station→LSOA, C: LSOA→acute)\n",
    "# What this does:\n",
    "# - Reshapes the cleaned long travel table into CSR sparse matrices.\n",
    "# - Optionally thins by a max-radius (minutes).\n",
    "# - Caches matrices + metadata for instant reloads next time.\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Tuple\n",
    "from scipy import sparse\n",
    "\n",
    "# Optional: set a max radius to thin pairs (e.g., 60.0). Use None to keep all pairs.\n",
    "MAX_RADIUS_MIN: float | None = None  # set to 60.0 if you want to prune long pairs\n",
    "\n",
    "def build_csr(\n",
    "    travel_df: pd.DataFrame,\n",
    "    row_codes: pd.Index,       # demand LSOAs (rows)\n",
    "    col_codes: pd.Index,       # site LSOAs for this leg (columns)\n",
    "    row_key: str,              # column in travel_df for rows\n",
    "    col_key: str,              # column in travel_df for cols\n",
    "    value_key: str = \"time_car_min\",\n",
    "    max_radius: float | None = MAX_RADIUS_MIN,\n",
    ") -> sparse.csr_matrix:\n",
    "    if len(col_codes) == 0:\n",
    "        return sparse.csr_matrix((len(row_codes), 0), dtype=np.float32)\n",
    "    # Filter to needed pairs\n",
    "    m = travel_df[row_key].isin(row_codes) & travel_df[col_key].isin(col_codes)\n",
    "    df = travel_df.loc[m, [row_key, col_key, value_key]].dropna(subset=[value_key]).copy()\n",
    "    if max_radius is not None:\n",
    "        df = df.loc[df[value_key] <= float(max_radius)].copy()\n",
    "    # Group duplicates to min\n",
    "    df = df.groupby([row_key, col_key], observed=True, sort=False)[value_key].min().reset_index()\n",
    "    # Integer maps for this matrix\n",
    "    rmap: Dict[str, int] = {c: i for i, c in enumerate(row_codes.astype(\"string\"))}\n",
    "    cmap: Dict[str, int] = {c: j for j, c in enumerate(col_codes.astype(\"string\"))}\n",
    "    rows = df[row_key].map(rmap).to_numpy(dtype=np.int32, na_value=-1)\n",
    "    cols = df[col_key].map(cmap).to_numpy(dtype=np.int32, na_value=-1)\n",
    "    vals = df[value_key].astype(np.float32).to_numpy()\n",
    "    good = (rows >= 0) & (cols >= 0) & np.isfinite(vals)\n",
    "    rows, cols, vals = rows[good], cols[good], vals[good]\n",
    "    mat = sparse.coo_matrix((vals, (rows, cols)), shape=(len(row_codes), len(col_codes)), dtype=np.float32).tocsr()\n",
    "    return mat\n",
    "\n",
    "# Build matrices\n",
    "R = build_csr(  # station → demand LSOA\n",
    "    travel_df=travel, row_codes=lsoa_index, col_codes=station_lsoas,\n",
    "    row_key=\"dest_lsoa\", col_key=\"origin_lsoa\", value_key=\"time_car_min\",\n",
    ")\n",
    "C = build_csr(  # demand LSOA → acute\n",
    "    travel_df=travel, row_codes=lsoa_index, col_codes=acute_lsoas,\n",
    "    row_key=\"origin_lsoa\", col_key=\"dest_lsoa\", value_key=\"time_car_min\",\n",
    ")\n",
    "\n",
    "# Report shapes and densities\n",
    "def _report(name: str, mat: sparse.csr_matrix):\n",
    "    m, n, nnz = mat.shape[0], mat.shape[1], mat.nnz\n",
    "    dens = (nnz / (m * n)) if (m > 0 and n > 0) else 0.0\n",
    "    print(f\"[OK] {name}: shape={mat.shape}, nnz={nnz:,}, density={dens:.4f}\")\n",
    "_report(\"R (station→LSOA)\", R)\n",
    "_report(\"C (LSOA→acute)\",  C)\n",
    "\n",
    "# Cache matrices & metadata\n",
    "sparse.save_npz(MATRICES_DIR / \"R_response_csr.npz\", R)\n",
    "sparse.save_npz(MATRICES_DIR / \"C_convey_csr.npz\",   C)\n",
    "np.savez(\n",
    "    MATRICES_DIR / \"matrix_metadata.npz\",\n",
    "    lsoa_index=lsoa_index.to_numpy(),\n",
    "    station_lsoas=station_lsoas.to_numpy(),\n",
    "    acute_lsoas=acute_lsoas.to_numpy(),\n",
    "    response_thresholds=np.array(RESPONSE_THRESHOLDS, dtype=np.int32),\n",
    "    convey_thresholds=np.array(SCENE_TO_AE_THRESHOLDS, dtype=np.int32),\n",
    "    blue_light=np.array([BLUE_LIGHT_FACTOR_RESPONSE, BLUE_LIGHT_FACTOR_CONVEY], dtype=np.float32),\n",
    "    max_radius=np.array([np.nan if MAX_RADIUS_MIN is None else float(MAX_RADIUS_MIN)], dtype=np.float32),\n",
    ")\n",
    "_ok(f\"Cached matrices & metadata → {MATRICES_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ba77c0-72d2-4272-b23c-ed64e8bda992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== STEP 3 SUMMARY ==\n",
      "t_resp: min=0.5, p25=5.7, med=11.22, p90=24.64, p95=28.76, max=92.22\n",
      "t_conv: min=0.5, p25=16.95, med=29.28, p90=74.47, p95=80.82, max=129.12\n",
      "Response KPIs:\n",
      "  threshold_min  pct_population\n",
      "             7           30.77\n",
      "            15           62.33\n",
      "            18           75.15\n",
      "            40           99.26\n",
      "Conveyance KPIs:\n",
      "  threshold_min  pct_population\n",
      "            30           51.77\n",
      "            45           69.35\n",
      "            60           79.03\n",
      "[OK] Step 3 complete — minima computed and KPIs exported.\n"
     ]
    }
   ],
   "source": [
    "# Step 3 — Vectorised minima, KPIs, and baseline exports\n",
    "# What this does:\n",
    "# - Computes nearest response/convey times (row-wise mins, no loops).\n",
    "# - Applies blue-light AFTER minima.\n",
    "# - Produces population-weighted coverage KPIs and exports tidy tables.\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Sequence\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Row-wise minimum in CSR without densifying\n",
    "def rowmin_csr(mat: csr_matrix) -> np.ndarray:\n",
    "    \"\"\"Return per-row minima; rows with no entries → +inf.\"\"\"\n",
    "    m = mat.shape[0]\n",
    "    if mat.shape[1] == 0:\n",
    "        return np.full(m, np.inf, dtype=np.float32)\n",
    "    mins = np.full(m, np.inf, dtype=np.float32)\n",
    "    indptr, data = mat.indptr, mat.data\n",
    "    for i in range(m):\n",
    "        start, end = indptr[i], indptr[i+1]\n",
    "        if start < end:\n",
    "            mins[i] = data[start:end].min()\n",
    "    return mins\n",
    "\n",
    "# Nearest times (use all columns for baseline)\n",
    "baseline_station_cols = np.arange(R.shape[1], dtype=np.int32)\n",
    "baseline_acute_cols   = np.arange(C.shape[1], dtype=np.int32)\n",
    "\n",
    "t_resp = rowmin_csr(R[:, baseline_station_cols]) * np.float32(BLUE_LIGHT_FACTOR_RESPONSE)\n",
    "t_conv = rowmin_csr(C[:, baseline_acute_cols])   * np.float32(BLUE_LIGHT_FACTOR_CONVEY)\n",
    "t_total = (t_resp + ON_SCENE_BUFFER_MIN + t_conv).astype(np.float32)\n",
    "\n",
    "# Pack into a tidy frame\n",
    "out_df = pd.DataFrame({\n",
    "    \"lsoa_code\": lsoa_index,\n",
    "    \"t_resp_min\": t_resp,\n",
    "    \"t_conv_min\": t_conv,\n",
    "    \"t_total_min\": t_total,\n",
    "}).set_index(\"lsoa_code\")\n",
    "\n",
    "# Boolean coverage flags for mapping/summary\n",
    "for thr in RESPONSE_THRESHOLDS:\n",
    "    out_df[f\"resp_le_{thr}\"] = (out_df[\"t_resp_min\"] <= thr).astype(\"uint8\")\n",
    "for thr in SCENE_TO_AE_THRESHOLDS:\n",
    "    out_df[f\"conv_le_{thr}\"] = (out_df[\"t_conv_min\"] <= thr).astype(\"uint8\")\n",
    "\n",
    "# KPI helper\n",
    "def coverage_table(times: pd.Series | np.ndarray, thresholds: Sequence[int], weights: pd.Series) -> pd.DataFrame:\n",
    "    arr = times.to_numpy() if isinstance(times, pd.Series) else np.asarray(times)\n",
    "    w = weights.reindex(lsoa_index).to_numpy(dtype=np.float64)\n",
    "    tot = w.sum() if w.sum() > 0 else 1.0\n",
    "    rows = []\n",
    "    for thr in thresholds:\n",
    "        covered = (arr <= thr)\n",
    "        rows.append({\"threshold_min\": int(thr), \"pct_population\": round(float((w*covered).sum()/tot*100.0), 2)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Compute KPIs\n",
    "resp_kpis = coverage_table(out_df[\"t_resp_min\"], RESPONSE_THRESHOLDS, population)\n",
    "conv_kpis = coverage_table(out_df[\"t_conv_min\"], SCENE_TO_AE_THRESHOLDS, population)\n",
    "\n",
    "# Exports\n",
    "times_path = TABLES_DIR / \"times_baseline.csv\"\n",
    "resp_kpi_path = TABLES_DIR / \"coverage_response_baseline.csv\"\n",
    "conv_kpi_path = TABLES_DIR / \"coverage_conveyance_baseline.csv\"\n",
    "by_lsoa_path = TABLES_DIR / \"coverage_by_lsoa_baseline.csv\"\n",
    "\n",
    "out_df.reset_index().to_csv(times_path, index=False)\n",
    "resp_kpis.to_csv(resp_kpi_path, index=False)\n",
    "conv_kpis.to_csv(conv_kpi_path, index=False)\n",
    "by_lsoa = out_df.copy()\n",
    "by_lsoa.insert(0, \"lsoa_code\", by_lsoa.index)\n",
    "by_lsoa[\"population\"] = population.reindex(by_lsoa.index).astype(int).to_numpy()\n",
    "by_lsoa.to_csv(by_lsoa_path, index=False)\n",
    "\n",
    "# Simple readout\n",
    "def _summ(name: str, arr: np.ndarray) -> str:\n",
    "    finite = np.isfinite(arr)\n",
    "    if not finite.any(): return f\"{name}: all inf\"\n",
    "    q = np.percentile(arr[finite], [0,25,50,90,95,100]).round(2)\n",
    "    return f\"{name}: min={q[0]}, p25={q[1]}, med={q[2]}, p90={q[3]}, p95={q[4]}, max={q[5]}\"\n",
    "\n",
    "print(\"\\n== STEP 3 SUMMARY ==\")\n",
    "print(_summ(\"t_resp\", t_resp))\n",
    "print(_summ(\"t_conv\", t_conv))\n",
    "print(\"Response KPIs:\\n\", resp_kpis.to_string(index=False))\n",
    "print(\"Conveyance KPIs:\\n\", conv_kpis.to_string(index=False))\n",
    "_ok(\"Step 3 complete — minima computed and KPIs exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e333d105-56ab-41d6-82e6-aaea65b83fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== STEP 4 SUMMARY ==\n",
      "scenario        leg  threshold_min  pct_population\n",
      "baseline conveyance             30           51.77\n",
      "baseline conveyance             45           69.35\n",
      "baseline conveyance             60           79.03\n",
      "baseline   response              7           30.77\n",
      "baseline   response             15           62.33\n",
      "baseline   response             18           75.15\n",
      "baseline   response             40           99.26\n",
      "[OK] Step 4 complete — scenarios run and KPIs exported.\n"
     ]
    }
   ],
   "source": [
    "# Step 4 — Scenario scaffold (define → run → export)\n",
    "# What this does:\n",
    "# - Lets you create “what-if” station sets (add/remove bases) via column selection.\n",
    "# - Reuses the same vectorised minima & KPI functions for instant diffs.\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Lookups for station/acute columns\n",
    "station_col_lookup: Dict[str, int] = {code: j for j, code in enumerate(station_lsoas)}\n",
    "acute_col_lookup:   Dict[str, int] = {code: j for j, code in enumerate(acute_lsoas)}\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Scenario:\n",
    "    name: str\n",
    "    station_cols: np.ndarray  # indices into R's columns\n",
    "    acute_cols:   np.ndarray  # indices into C's columns\n",
    "\n",
    "def run_scenario(scn: Scenario) -> dict[str, pd.DataFrame]:\n",
    "    t_r = rowmin_csr(R[:, scn.station_cols]) * np.float32(BLUE_LIGHT_FACTOR_RESPONSE)\n",
    "    t_c = rowmin_csr(C[:, scn.acute_cols])   * np.float32(BLUE_LIGHT_FACTOR_CONVEY)\n",
    "    times = pd.DataFrame({\"lsoa_code\": lsoa_index, \"t_resp_min\": t_r, \"t_conv_min\": t_c})\n",
    "    return {\n",
    "        \"times\": times,\n",
    "        \"resp_kpis\": coverage_table(times.set_index(\"lsoa_code\")[\"t_resp_min\"], RESPONSE_THRESHOLDS, population),\n",
    "        \"conv_kpis\": coverage_table(times.set_index(\"lsoa_code\")[\"t_conv_min\"], SCENE_TO_AE_THRESHOLDS, population),\n",
    "    }\n",
    "\n",
    "# Baseline scenario (all current columns)\n",
    "SCENARIOS = [\n",
    "    Scenario(\"baseline\",\n",
    "             station_cols=np.arange(R.shape[1], dtype=np.int32),\n",
    "             acute_cols=np.arange(C.shape[1], dtype=np.int32)),\n",
    "    # Example: add a station by code (uncomment & replace):\n",
    "    # Scenario(\"add_E01XXXXXX\",\n",
    "    #          station_cols=np.sort(np.unique(np.r_[np.arange(R.shape[1]), station_col_lookup[\"E01XXXXXX\"]])).astype(np.int32),\n",
    "    #          acute_cols=np.arange(C.shape[1], dtype=np.int32)),\n",
    "]\n",
    "\n",
    "# Run & export\n",
    "rows = []\n",
    "for scn in SCENARIOS:\n",
    "    res = run_scenario(scn)\n",
    "    res[\"times\"].to_csv(TABLES_DIR / f\"times_{scn.name}.csv\", index=False)\n",
    "    for _, r in res[\"resp_kpis\"].iterrows():\n",
    "        rows.append({\"scenario\": scn.name, \"leg\": \"response\", \"threshold_min\": int(r[\"threshold_min\"]), \"pct_population\": float(r[\"pct_population\"])})\n",
    "    for _, r in res[\"conv_kpis\"].iterrows():\n",
    "        rows.append({\"scenario\": scn.name, \"leg\": \"conveyance\", \"threshold_min\": int(r[\"threshold_min\"]), \"pct_population\": float(r[\"pct_population\"])})\n",
    "\n",
    "scen_kpis = pd.DataFrame(rows).sort_values([\"scenario\", \"leg\", \"threshold_min\"])\n",
    "scen_kpis.to_csv(TABLES_DIR / \"scenario_kpis.csv\", index=False)\n",
    "print(\"\\n== STEP 4 SUMMARY ==\")\n",
    "print(scen_kpis.to_string(index=False))\n",
    "_ok(\"Step 4 complete — scenarios run and KPIs exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2012c70b-49a7-40ae-aca0-762df60e5ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] No non-baseline scenarios defined. Wrote empty scenario_delta_summary.csv.\n",
      "[OK] Step 5 complete — scenario diffs & QA exported.\n"
     ]
    }
   ],
   "source": [
    "# Step 5 — Scenario diffs, exports, and QA\n",
    "# What this does:\n",
    "# - Compares every non-baseline scenario against baseline (minute-saved deltas).\n",
    "# - Reports population-weighted avg minutes saved and KPI percentage-point shifts.\n",
    "# - Flags any LSOAs where response gets worse (shouldn’t when adding stations).\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Sequence\n",
    "\n",
    "# Helper: compute times for a given set of columns\n",
    "def _scenario_times(station_cols: np.ndarray, acute_cols: np.ndarray) -> pd.DataFrame:\n",
    "    t_r = rowmin_csr(R[:, station_cols]) * np.float32(BLUE_LIGHT_FACTOR_RESPONSE)\n",
    "    t_c = rowmin_csr(C[:, acute_cols])   * np.float32(BLUE_LIGHT_FACTOR_CONVEY)\n",
    "    t_tot = (t_r + ON_SCENE_BUFFER_MIN + t_c).astype(np.float32)\n",
    "    return pd.DataFrame(\n",
    "        {\"lsoa_code\": lsoa_index, \"t_resp_min\": t_r, \"t_conv_min\": t_c, \"t_total_min\": t_tot}\n",
    "    )\n",
    "\n",
    "# KPI helper (re-use from Step 3 if present)\n",
    "def _coverage_kpis(times_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    r = coverage_table(times_df.set_index(\"lsoa_code\")[\"t_resp_min\"], RESPONSE_THRESHOLDS, population)\n",
    "    c = coverage_table(times_df.set_index(\"lsoa_code\")[\"t_conv_min\"], SCENE_TO_AE_THRESHOLDS, population)\n",
    "    return r, c\n",
    "\n",
    "# Baseline artefacts\n",
    "baseline_times = _scenario_times(\n",
    "    station_cols=np.arange(R.shape[1], dtype=np.int32),\n",
    "    acute_cols=np.arange(C.shape[1], dtype=np.int32),\n",
    ")\n",
    "base_resp_kpi, base_conv_kpi = _coverage_kpis(baseline_times)\n",
    "\n",
    "# Filter to non-baseline scenarios\n",
    "scenarios_to_run = [s for s in SCENARIOS if s.name != \"baseline\"]\n",
    "\n",
    "rows_summary: list[Dict] = []\n",
    "if len(scenarios_to_run) == 0:\n",
    "    # Write an empty summary so downstream steps don’t break\n",
    "    empty_summary = pd.DataFrame(\n",
    "        columns=[\"scenario\", \"w_mean_resp_minutes_saved\", \"w_mean_total_minutes_saved\", \"n_worsened_resp\"]\n",
    "    )\n",
    "    empty_summary.to_csv(TABLES_DIR / \"scenario_delta_summary.csv\", index=False)\n",
    "    print(\"[WARN] No non-baseline scenarios defined. Wrote empty scenario_delta_summary.csv.\")\n",
    "else:\n",
    "    for scn in scenarios_to_run:\n",
    "        scn_times = _scenario_times(scn.station_cols, scn.acute_cols)\n",
    "\n",
    "        # Per-LSOA deltas (positive = minutes saved vs baseline)\n",
    "        merged = baseline_times.merge(scn_times, on=\"lsoa_code\", suffixes=(\"_base\", \"_scn\"))\n",
    "        merged[\"d_resp_min\"]  = merged[\"t_resp_min_base\"]  - merged[\"t_resp_min_scn\"]\n",
    "        merged[\"d_conv_min\"]  = merged[\"t_conv_min_base\"]  - merged[\"t_conv_min_scn\"]\n",
    "        merged[\"d_total_min\"] = merged[\"t_total_min_base\"] - merged[\"t_total_min_scn\"]\n",
    "\n",
    "        # QA: response should not worsen when adding stations\n",
    "        worsened = int((merged[\"d_resp_min\"] < -1e-6).sum())\n",
    "        if worsened:\n",
    "            print(f\"[WARN] {scn.name}: {worsened} LSOAs have worse response times than baseline.\")\n",
    "\n",
    "        # KPI deltas (percentage-point change)\n",
    "        scn_resp_kpi, scn_conv_kpi = _coverage_kpis(scn_times)\n",
    "        resp_delta = scn_resp_kpi.merge(base_resp_kpi, on=\"threshold_min\", suffixes=(\"_scn\", \"_base\"))\n",
    "        resp_delta[\"delta_pp\"] = resp_delta[\"pct_population_scn\"] - resp_delta[\"pct_population_base\"]\n",
    "        conv_delta = scn_conv_kpi.merge(base_conv_kpi, on=\"threshold_min\", suffixes=(\"_scn\", \"_base\"))\n",
    "        conv_delta[\"delta_pp\"] = conv_delta[\"pct_population_scn\"] - conv_delta[\"pct_population_base\"]\n",
    "\n",
    "        # Population-weighted average minutes saved\n",
    "        pop = population.reindex(merged[\"lsoa_code\"]).to_numpy(dtype=float)\n",
    "        tot_pop = pop.sum() if pop.sum() > 0 else 1.0\n",
    "        w_mean_resp_save  = float((pop * merged[\"d_resp_min\"].to_numpy()).sum() / tot_pop)\n",
    "        w_mean_total_save = float((pop * merged[\"d_total_min\"].to_numpy()).sum() / tot_pop)\n",
    "\n",
    "        rows_summary.append(\n",
    "            {\n",
    "                \"scenario\": scn.name,\n",
    "                \"w_mean_resp_minutes_saved\": round(w_mean_resp_save, 3),\n",
    "                \"w_mean_total_minutes_saved\": round(w_mean_total_save, 3),\n",
    "                \"n_worsened_resp\": worsened,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Exports per scenario\n",
    "        merged.to_csv(TABLES_DIR / f\"delta_by_lsoa_{scn.name}.csv\", index=False)\n",
    "        resp_delta.to_csv(TABLES_DIR / f\"delta_kpi_response_{scn.name}.csv\", index=False)\n",
    "        conv_delta.to_csv(TABLES_DIR / f\"delta_kpi_convey_{scn.name}.csv\", index=False)\n",
    "\n",
    "        print(f\"[OK] {scn.name}: wrote delta tables (by_lsoa / kpis).\")\n",
    "\n",
    "    # Consolidated scenario summary\n",
    "    summary_df = pd.DataFrame(rows_summary).sort_values(\"scenario\")\n",
    "    summary_df.to_csv(TABLES_DIR / \"scenario_delta_summary.csv\", index=False)\n",
    "    print(\"[OK] Scenario delta summary →\", TABLES_DIR / \"scenario_delta_summary.csv\")\n",
    "\n",
    "print(\"[OK] Step 5 complete — scenario diffs & QA exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08327bf3-59aa-4328-856c-ad7e0f262f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Step 6 complete — maps written:\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_response_le_7min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_response_le_15min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_response_le_18min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_response_le_40min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_conveyance_le_30min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_conveyance_le_45min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_conveyance_le_60min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_t_resp_min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_t_conv_min.png\n",
      " - /Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)/GitHub Repo/REACH-Map-NHS-SW/data/raw/test_data_ICB_level/maps/map_t_total_min.png\n"
     ]
    }
   ],
   "source": [
    "# Step 6 — Maps (binary coverage + continuous layers)\n",
    "# What this does:\n",
    "# - Builds simple choropleths for response/convey thresholds and continuous time surfaces.\n",
    "# - Overlays station/acute centroids (derived from LSOA polygons).\n",
    "# - Saves PNGs under maps/ for slides or dashboards.\n",
    "\n",
    "from __future__ import annotations\n",
    "import contextlib\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Join mapping geometry with times/flags from Step 3\n",
    "gmap = lsoa_g.join(out_df, how=\"left\")\n",
    "\n",
    "# Centroids for station/acute LSOAs (EPSG:27700 → safe for centroid)\n",
    "centroids = gmap.geometry.centroid\n",
    "station_pts = gpd.GeoDataFrame({\"lsoa_code\": station_lsoas}, geometry=centroids.reindex(station_lsoas), crs=gmap.crs)\n",
    "acute_pts   = gpd.GeoDataFrame({\"lsoa_code\": acute_lsoas},   geometry=centroids.reindex(acute_lsoas),   crs=gmap.crs)\n",
    "\n",
    "# Colours\n",
    "COVERED_COLOUR = \"#2ca25f\"\n",
    "UNCOVERED_COLOUR = \"#de2d26\"\n",
    "BORDER_COLOUR = \"#ffffff\"\n",
    "BG_COLOUR = \"#f7f7f7\"\n",
    "PTS_STATION_COLOUR = \"#1f78b4\"\n",
    "PTS_ACUTE_COLOUR = \"#6a3d9a\"\n",
    "\n",
    "def _legend_binary(ax, covered_label=\"Covered\", uncovered_label=\"Not covered\"):\n",
    "    patches = [\n",
    "        Patch(facecolor=COVERED_COLOUR, edgecolor=BORDER_COLOUR, label=covered_label),\n",
    "        Patch(facecolor=UNCOVERED_COLOUR, edgecolor=BORDER_COLOUR, label=uncovered_label),\n",
    "        Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=PTS_STATION_COLOUR, markersize=8, label=\"Station\"),\n",
    "        Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=PTS_ACUTE_COLOUR, markersize=8, label=\"Acute\"),\n",
    "    ]\n",
    "    ax.legend(handles=patches, loc=\"lower left\", frameon=True, framealpha=0.9)\n",
    "\n",
    "def _plot_binary(layer_col: str, title: str, outfile: Path):\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 9), dpi=150, facecolor=\"white\")\n",
    "    ax.set_facecolor(BG_COLOUR)\n",
    "    if layer_col not in gmap.columns:\n",
    "        ax.text(0.5, 0.5, f\"Column '{layer_col}' not found.\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "    else:\n",
    "        covered = gmap[gmap[layer_col] == 1]\n",
    "        not_cov = gmap[gmap[layer_col] != 1]\n",
    "        with contextlib.suppress(Exception):\n",
    "            not_cov.plot(ax=ax, color=UNCOVERED_COLOUR, edgecolor=BORDER_COLOUR, linewidth=0.2, rasterized=True)\n",
    "            covered.plot(ax=ax, color=COVERED_COLOUR, edgecolor=BORDER_COLOUR, linewidth=0.2, rasterized=True)\n",
    "        if not station_pts.empty:\n",
    "            station_pts.plot(ax=ax, markersize=10, color=PTS_STATION_COLOUR, alpha=0.9)\n",
    "        if not acute_pts.empty:\n",
    "            acute_pts.plot(ax=ax, markersize=10, color=PTS_ACUTE_COLOUR, alpha=0.9)\n",
    "        _legend_binary(ax)\n",
    "    ax.set_title(title, fontsize=13, pad=10)\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def _plot_continuous(value_col: str, title: str, outfile: Path, vmin: float | None = None, vmax: float | None = None):\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 9), dpi=150, facecolor=\"white\")\n",
    "    ax.set_facecolor(BG_COLOUR)\n",
    "    if value_col not in gmap.columns:\n",
    "        ax.text(0.5, 0.5, f\"Column '{value_col}' not found.\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "    else:\n",
    "        data = gmap[value_col].replace([np.inf, -np.inf], np.nan)\n",
    "        if vmin is None: vmin = float(np.nanpercentile(data, 2)) if np.isfinite(data).any() else 0.0\n",
    "        if vmax is None: vmax = float(np.nanpercentile(data, 98)) if np.isfinite(data).any() else 1.0\n",
    "        vmin, vmax = (min(vmin, vmax), max(vmin, vmax))\n",
    "        with contextlib.suppress(Exception):\n",
    "            gmap.plot(column=value_col, ax=ax, cmap=\"viridis\", vmin=vmin, vmax=vmax,\n",
    "                      edgecolor=BORDER_COLOUR, linewidth=0.2, legend=True,\n",
    "                      legend_kwds={\"label\": \"Minutes\", \"shrink\": 0.6}, rasterized=True)\n",
    "        if not station_pts.empty:\n",
    "            station_pts.plot(ax=ax, markersize=10, color=PTS_STATION_COLOUR, alpha=0.9)\n",
    "        if not acute_pts.empty:\n",
    "            acute_pts.plot(ax=ax, markersize=10, color=PTS_ACUTE_COLOUR, alpha=0.9)\n",
    "    ax.set_title(title, fontsize=13, pad=10)\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# Generate maps\n",
    "written = []\n",
    "for thr in RESPONSE_THRESHOLDS:\n",
    "    f = MAPS_DIR / f\"map_response_le_{thr}min.png\"\n",
    "    _plot_binary(f\"resp_le_{thr}\", f\"Response coverage ≤{thr} min\", f)\n",
    "    written.append(f)\n",
    "for thr in SCENE_TO_AE_THRESHOLDS:\n",
    "    f = MAPS_DIR / f\"map_conveyance_le_{thr}min.png\"\n",
    "    _plot_binary(f\"conv_le_{thr}\", f\"Conveyance coverage ≤{thr} min\", f)\n",
    "    written.append(f)\n",
    "\n",
    "f = MAPS_DIR / \"map_t_resp_min.png\"\n",
    "_plot_continuous(\"t_resp_min\", \"Nearest response time (min)\", f); written.append(f)\n",
    "f = MAPS_DIR / \"map_t_conv_min.png\"\n",
    "_plot_continuous(\"t_conv_min\", \"Nearest conveyance time (min)\", f); written.append(f)\n",
    "if \"t_total_min\" in gmap.columns:\n",
    "    f = MAPS_DIR / \"map_t_total_min.png\"\n",
    "    _plot_continuous(\"t_total_min\", \"End-to-end (resp + scene + convey)\", f); written.append(f)\n",
    "\n",
    "print(\"[OK] Step 6 complete — maps written:\")\n",
    "for p in written:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18478e2-bac4-4063-81e1-0deeba721ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== STEP 7 — Parity check ==\n",
      "Response parity: PASS (max abs diff = 0.000000 min)\n",
      "Conveyance parity: PASS (max abs diff = 0.000000 min)\n",
      "No-route sentinels → response: 0, conveyance: 0\n",
      "[OK] Step 7 complete — validation run finished.\n"
     ]
    }
   ],
   "source": [
    "# Step 7 — Validation (parity vs group-by) and data sentinels\n",
    "# What this does:\n",
    "# - Recomputes nearest times using plain pandas group-bys (02a style).\n",
    "# - Confirms the matrix minima match (within tiny tolerance).\n",
    "# - Reports any +∞ rows (no-route) per leg.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "ATOL = 1e-6  # numeric tolerance for parity\n",
    "\n",
    "# 7.1 Response parity (station→LSOA)\n",
    "resp_groupby = (\n",
    "    travel[travel[\"origin_lsoa\"].isin(station_lsoas)]\n",
    "    .groupby(\"dest_lsoa\", observed=True)[\"time_car_min\"]\n",
    "    .min()\n",
    "    .reindex(lsoa_index)\n",
    "    .astype(\"float32\")\n",
    "    .to_numpy()\n",
    ") * np.float32(BLUE_LIGHT_FACTOR_RESPONSE)\n",
    "\n",
    "# 7.2 Conveyance parity (LSOA→acute)\n",
    "conv_groupby = (\n",
    "    travel[travel[\"dest_lsoa\"].isin(acute_lsoas)]\n",
    "    .groupby(\"origin_lsoa\", observed=True)[\"time_car_min\"]\n",
    "    .min()\n",
    "    .reindex(lsoa_index)\n",
    "    .astype(\"float32\")\n",
    "    .to_numpy()\n",
    ") * np.float32(BLUE_LIGHT_FACTOR_CONVEY)\n",
    "\n",
    "# 7.3 Compare to matrix-derived\n",
    "resp_diff = np.nanmax(np.abs(resp_groupby - out_df[\"t_resp_min\"].to_numpy()))\n",
    "conv_diff = np.nanmax(np.abs(conv_groupby - out_df[\"t_conv_min\"].to_numpy()))\n",
    "resp_ok = bool(np.allclose(resp_groupby, out_df[\"t_resp_min\"].to_numpy(), atol=ATOL, equal_nan=True))\n",
    "conv_ok = bool(np.allclose(conv_groupby, out_df[\"t_conv_min\"].to_numpy(), atol=ATOL, equal_nan=True))\n",
    "\n",
    "print(\"\\n== STEP 7 — Parity check ==\")\n",
    "print(f\"Response parity: {'PASS' if resp_ok else 'FAIL'} (max abs diff = {resp_diff:.6f} min)\")\n",
    "print(f\"Conveyance parity: {'PASS' if conv_ok else 'FAIL'} (max abs diff = {conv_diff:.6f} min)\")\n",
    "\n",
    "# 7.4 Sentinel report for +∞ (no-route) rows\n",
    "n_inf_resp = int(np.isinf(out_df['t_resp_min'].to_numpy()).sum())\n",
    "n_inf_conv = int(np.isinf(out_df['t_conv_min'].to_numpy()).sum())\n",
    "print(f\"No-route sentinels → response: {n_inf_resp}, conveyance: {n_inf_conv}\")\n",
    "print(\"[OK] Step 7 complete — validation run finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8744f8a0-6440-4916-a7d4-30e630c2e2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Step 8 ready — use make_add_station_scenario(...) or make_remove_station_scenario(...) to extend SCENARIOS.\n"
     ]
    }
   ],
   "source": [
    "# Step 8 — Convenience: scenario helpers (add/remove stations) and quick runner\n",
    "# What this does:\n",
    "# - Adds ergonomic helpers to build scenarios by LSOA code(s).\n",
    "# - Avoids manual index math; validates input codes.\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Sequence\n",
    "\n",
    "# Lookups (if not already created)\n",
    "station_col_lookup = {code: j for j, code in enumerate(station_lsoas)}\n",
    "acute_col_lookup   = {code: j for j, code in enumerate(acute_lsoas)}\n",
    "\n",
    "def make_add_station_scenario(name: str, add_station_codes: Sequence[str]) -> Scenario:\n",
    "    missing = [c for c in add_station_codes if c not in station_col_lookup]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Unknown station LSOA codes: {missing}\")\n",
    "    add_cols = np.array([station_col_lookup[c] for c in add_station_codes], dtype=np.int32)\n",
    "    base_cols = np.arange(R.shape[1], dtype=np.int32)\n",
    "    new_cols = np.sort(np.unique(np.r_[base_cols, add_cols])).astype(np.int32)\n",
    "    return Scenario(name=name, station_cols=new_cols, acute_cols=np.arange(C.shape[1], dtype=np.int32))\n",
    "\n",
    "def make_remove_station_scenario(name: str, remove_station_codes: Sequence[str]) -> Scenario:\n",
    "    missing = [c for c in remove_station_codes if c not in station_col_lookup]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Unknown station LSOA codes: {missing}\")\n",
    "    base_cols = np.arange(R.shape[1], dtype=np.int32)\n",
    "    remove_cols = np.array([station_col_lookup[c] for c in remove_station_codes], dtype=np.int32)\n",
    "    keep_mask = ~np.isin(base_cols, remove_cols)\n",
    "    return Scenario(name=name, station_cols=base_cols[keep_mask], acute_cols=np.arange(C.shape[1], dtype=np.int32))\n",
    "\n",
    "print(\"[OK] Step 8 ready — use make_add_station_scenario(...) or make_remove_station_scenario(...) to extend SCENARIOS.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
