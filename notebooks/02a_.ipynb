{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656c70b6-c809-471a-b9fc-451662eeba8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5302420-5a3a-47b2-a9ac-e5be15247453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 — Setup, paths, helpers\n",
    "# - Uses ONLY files present in your test_data_ICB_level folder.\n",
    "# - Creates tables/maps/matrices subfolders if missing.\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional\n",
    "from datetime import datetime, timezone\n",
    "import warnings, json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "\n",
    "# >>> UPDATE PATH IF YOU MOVE THE FOLDER <<<\n",
    "BASE = Path(\"/Users/rosstaylor/Downloads/Code Repositories/REACH Map (NHS SW)\") \\\n",
    "    / \"GitHub Repo\" / \"REACH-Map-NHS-SW\" / \"data\" / \"raw\" / \"test_data_ICB_level\"\n",
    "\n",
    "TABLES   = BASE / \"tables\";   TABLES.mkdir(parents=True, exist_ok=True)\n",
    "MATRICES = BASE / \"matrices\"; MATRICES.mkdir(parents=True, exist_ok=True)\n",
    "MAPS     = BASE / \"maps\";     MAPS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RESPONSE_THRESHOLDS      = (7, 15, 18, 40)\n",
    "SCENE_TO_AE_THRESHOLDS   = (30, 45, 60)\n",
    "\n",
    "RUN_META = {\n",
    "    \"notebook\": \"02a_coverage\",\n",
    "    \"utc\": datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"base\": str(BASE),\n",
    "    \"resp_thresholds\": RESPONSE_THRESHOLDS,\n",
    "    \"conv_thresholds\": SCENE_TO_AE_THRESHOLDS,\n",
    "}\n",
    "\n",
    "def _ok(msg: str) -> None: print(f\"[OK] {msg}\")\n",
    "def _warn(msg: str) -> None: warnings.warn(msg, stacklevel=2)\n",
    "def _die(msg: str) -> None: raise RuntimeError(msg)\n",
    "\n",
    "def _save_prov(target: Path, extras: dict | None = None) -> None:\n",
    "    prov = dict(RUN_META); prov.update(extras or {})\n",
    "    target.with_suffix(target.suffix + \".provenance.json\").write_text(\n",
    "        json.dumps(prov, indent=2)\n",
    "    )\n",
    "\n",
    "def _choose_col(df: pd.DataFrame, candidates: Iterable[str]) -> str:\n",
    "    for c in candidates:\n",
    "        if c in df.columns: return c\n",
    "    _die(f\"None of {list(candidates)} present. Found: {list(df.columns)[:12]}\")\n",
    "\n",
    "def _check_monotone(counts_by_t: dict[int, int]) -> None:\n",
    "    vals = [counts_by_t[t] for t in sorted(counts_by_t)]\n",
    "    if any(b < a for a, b in zip(vals, vals[1:])):\n",
    "        _die(\"Coverage counts are not monotone with increasing thresholds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d431e6b5-a8ac-43ed-b63e-344158342744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] LSOA universe=336 | CRS=EPSG:27700\n"
     ]
    }
   ],
   "source": [
    "# Step 1 — LSOA universe + geometry\n",
    "\n",
    "LOOKUP_GPKG = BASE / \"cornwall_icb_lsoa_lookup.gpkg\"\n",
    "LOOKUP_CSV  = BASE / \"cornwall_icb_lsoa_lookup.csv\"\n",
    "\n",
    "def _read_lookup_gpkg(gpkg: Path) -> gpd.GeoDataFrame:\n",
    "    # Try named layer; else grab the first available\n",
    "    layer_name = \"cornwall_icb_lsoa_lookup\"\n",
    "    if gpkg.exists():\n",
    "        try:\n",
    "            layers = fiona.listlayers(gpkg)\n",
    "            lyr = layer_name if layer_name in layers else layers[0]\n",
    "            g = gpd.read_file(gpkg, layer=lyr)\n",
    "            return g\n",
    "        except Exception as e:\n",
    "            _warn(f\"GPKG read failed ({e}); will try CSV fallback.\")\n",
    "    return None\n",
    "\n",
    "lsoa_g = _read_lookup_gpkg(LOOKUP_GPKG)\n",
    "if lsoa_g is None:\n",
    "    if not LOOKUP_CSV.exists(): _die(\"No lookup GPKG/CSV found.\")\n",
    "    lsoa_g = gpd.read_file(LOOKUP_CSV)  # will be a plain DF; geometry absent\n",
    "    if \"geometry\" not in lsoa_g.columns:\n",
    "        # No geometry in CSV → keep attributes only; we’ll skip maps if so\n",
    "        lsoa_g = pd.read_csv(LOOKUP_CSV, dtype={\"lsoa_code\":\"string\"}).pipe(pd.DataFrame)\n",
    "\n",
    "if \"lsoa_code\" not in lsoa_g.columns:\n",
    "    _die(\"Lookup lacks 'lsoa_code'.\")\n",
    "\n",
    "lsoa_g[\"lsoa_code\"] = lsoa_g[\"lsoa_code\"].astype(\"string\")\n",
    "lsoa_g = lsoa_g.drop_duplicates(subset=[\"lsoa_code\"]).reset_index(drop=True)\n",
    "lsoa_index = lsoa_g[\"lsoa_code\"].tolist()\n",
    "_ok(f\"LSOA universe={len(lsoa_index):,} | CRS={getattr(lsoa_g, 'crs', None)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "486771ca-219e-432b-a0fc-4e9371f0880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — Canonical population parquet\n",
    "\n",
    "POP_PARQUET = TABLES / \"population_by_lsoa.parquet\"\n",
    "GH_CSV      = BASE / \"demographics_general_health_icb.csv\"\n",
    "\n",
    "def _standardise_population(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"lsoa_code\"] = df[\"lsoa_code\"].astype(\"string\")\n",
    "    # Prefer 'population_total'; fall back to 'population'; else sum numerics\n",
    "    if \"population_total\" in df.columns:\n",
    "        out = df.rename(columns={\"population_total\": \"population\"})[[\"lsoa_code\",\"population\"]].copy()\n",
    "    elif \"population\" in df.columns:\n",
    "        out = df[[\"lsoa_code\",\"population\"]].copy()\n",
    "    else:\n",
    "        num_cols = [c for c in df.select_dtypes(include=\"number\").columns if c != \"lsoa_code\"]\n",
    "        if not num_cols: _die(\"No numeric columns to infer population from.\")\n",
    "        out = df.assign(population=df[num_cols].sum(axis=1, skipna=True))[[\"lsoa_code\",\"population\"]].copy()\n",
    "    out.loc[:, \"population\"] = pd.to_numeric(out[\"population\"], errors=\"coerce\")\n",
    "    if out[\"population\"].isna().any(): _die(\"Population has NaNs after standardisation.\")\n",
    "    return out\n",
    "\n",
    "if POP_PARQUET.exists():\n",
    "    population = pd.read_parquet(POP_PARQUET)\n",
    "    population[\"lsoa_code\"] = population[\"lsoa_code\"].astype(\"string\")\n",
    "else:\n",
    "    if not GH_CSV.exists(): _die(\"Missing demographics_general_health_icb.csv.\")\n",
    "    population = pd.read_csv(GH_CSV, dtype={\"lsoa_code\":\"string\"})\n",
    "    population = _standardise_population(population)\n",
    "    # Align to universe and persist\n",
    "    population = pd.DataFrame({\"lsoa_code\": lsoa_index}).merge(population, on=\"lsoa_code\", how=\"left\")\n",
    "    if population[\"population\"].isna().any(): _die(\"Population missing for some LSOAs.\")\n",
    "    population.to_parquet(POP_PARQUET, index=False)\n",
    "    _save_prov(POP_PARQUET, {\"source\":\"02a_standardised\"})\n",
    "    _ok(f\"Wrote {POP_PARQUET.name} | rows={len(population):,} | sum={int(population['population'].sum()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e6a9270-6fb0-452f-8f83-82bb691ea0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Equity parquet (IMD quintile + RUC)\n",
    "\n",
    "EQUITY_PARQUET = TABLES / \"lsoa_lookup_equity.parquet\"\n",
    "IMD_CSV = BASE / \"imd_icb.csv\"\n",
    "RUC_CSV = BASE / \"ruc_icb.csv\"\n",
    "\n",
    "def _build_equity() -> pd.DataFrame:\n",
    "    eq = pd.DataFrame({\"lsoa_code\": lsoa_index})\n",
    "    # Keep optional labels if present in lookup\n",
    "    for src, dst in ((\"ladnm\",\"lad_name\"), (\"icb_name\",\"icb_name\")):\n",
    "        if src in lsoa_g.columns:\n",
    "            eq = eq.merge(lsoa_g[[\"lsoa_code\", src]].rename(columns={src: dst}), on=\"lsoa_code\", how=\"left\")\n",
    "\n",
    "    if not IMD_CSV.exists(): _die(\"Missing imd_icb.csv.\")\n",
    "    imd = pd.read_csv(IMD_CSV, dtype={\"lsoa_code\":\"string\"})\n",
    "    imd_col = next((c for c in (\"imd19\",\"IMD2019_Rank\",\"imd_rank\") if c in imd.columns), None)\n",
    "    if imd_col is None: _die(\"IMD file lacks an IMD rank column (imd19/IMD2019_Rank/imd_rank).\")\n",
    "    imd = imd[[\"lsoa_code\", imd_col]].rename(columns={imd_col:\"imd_rank\"})\n",
    "    imd[\"imd_rank\"] = pd.to_numeric(imd[\"imd_rank\"], errors=\"coerce\")\n",
    "    imd = imd.sort_values(\"imd_rank\")\n",
    "    imd[\"imd_quintile\"] = pd.qcut(imd[\"imd_rank\"].rank(method=\"first\"), 5, labels=[1,2,3,4,5]).astype(\"Int64\")\n",
    "    imd = imd[[\"lsoa_code\",\"imd_quintile\"]]\n",
    "\n",
    "    if not RUC_CSV.exists(): _die(\"Missing ruc_icb.csv.\")\n",
    "    ruc = pd.read_csv(RUC_CSV, dtype={\"lsoa_code\":\"string\"})\n",
    "    ruc_col = next((c for c in (\"ruc21nm\",\"urban_rural_flag\",\"ruc_label\") if c in ruc.columns), None)\n",
    "    if ruc_col is None: _die(\"RUC file lacks ruc21nm/urban_rural_flag.\")\n",
    "    ruc = ruc[[\"lsoa_code\", ruc_col]].rename(columns={ruc_col:\"ruc_category\"})\n",
    "    return (eq.merge(imd, on=\"lsoa_code\", how=\"left\")\n",
    "             .merge(ruc, on=\"lsoa_code\", how=\"left\"))\n",
    "\n",
    "if EQUITY_PARQUET.exists():\n",
    "    equity = pd.read_parquet(EQUITY_PARQUET)\n",
    "else:\n",
    "    equity = _build_equity()\n",
    "    equity.to_parquet(EQUITY_PARQUET, index=False)\n",
    "    _save_prov(EQUITY_PARQUET, {\"source\":\"02a_built\"})\n",
    "    _ok(f\"Wrote {EQUITY_PARQUET.name} | rows={len(equity):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67caa3e8-1c6c-48a8-b8b6-207cc161ca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] ambulance_stations_icb.csv: using column 'lsoa21cd' → 14 LSOAs\n",
      "[OK] acute_hospitals_icb.csv: using column 'lsoa21cd' → 3 LSOAs\n",
      "[OK] Baseline mins → baseline_min_times.npz | resp(mean)=12.9 | conv(mean)=36.4\n"
     ]
    }
   ],
   "source": [
    "# Step 4 — Travel + sites → baseline min times (lsoa21cd-aware)\n",
    "\n",
    "TRAVEL_CSV   = BASE / \"travel_matrix_lsoa_icb.csv\"\n",
    "STATIONS_CSV = BASE / \"ambulance_stations_icb.csv\"\n",
    "ACUTE_CSV    = BASE / \"acute_hospitals_icb.csv\"\n",
    "\n",
    "if not TRAVEL_CSV.exists(): _die(f\"Missing {TRAVEL_CSV.name}\")\n",
    "if not STATIONS_CSV.exists(): _die(f\"Missing {STATIONS_CSV.name}\")\n",
    "if not ACUTE_CSV.exists(): _die(f\"Missing {ACUTE_CSV.name}\")\n",
    "\n",
    "# Travel matrix (origin_lsoa, dest_lsoa, time_* → time_car_min)\n",
    "travel = pd.read_csv(\n",
    "    TRAVEL_CSV,\n",
    "    dtype={\"origin_lsoa\": \"string\", \"dest_lsoa\": \"string\"},\n",
    ")\n",
    "time_col = _choose_col(\n",
    "    travel, (\"time_car_min\", \"time_min\", \"minutes\", \"drive_min\", \"t_min\")\n",
    ")\n",
    "if time_col != \"time_car_min\":\n",
    "    travel = travel.rename(columns={time_col: \"time_car_min\"})\n",
    "travel[\"time_car_min\"] = travel[\"time_car_min\"].astype(\"float32\")\n",
    "\n",
    "def _load_site_codes(path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load site LSOA codes from CSV, preferring 'lsoa21cd', with fallbacks.\n",
    "    Accepted cols: lsoa21cd, lsoa_code, lsoa11cd, LSOA, site_lsoa, lsoa\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    col = next(\n",
    "        (c for c in (\n",
    "            \"lsoa21cd\", \"lsoa_code\", \"lsoa11cd\", \"LSOA\", \"site_lsoa\", \"lsoa\"\n",
    "        ) if c in df.columns),\n",
    "        None,\n",
    "    )\n",
    "    if col is None:\n",
    "        _die(f\"{path.name} lacks an LSOA column (expect one of \"\n",
    "             f\"lsoa21cd/lsoa_code/lsoa11cd/LSOA/site_lsoa/lsoa).\")\n",
    "    codes = (\n",
    "        df[col]\n",
    "        .astype(\"string\")\n",
    "        .str.strip()\n",
    "        .dropna()\n",
    "        .unique()\n",
    "    )\n",
    "    _ok(f\"{path.name}: using column '{col}' → {len(codes)} LSOAs\")\n",
    "    return codes\n",
    "\n",
    "station_lsoas = _load_site_codes(STATIONS_CSV)\n",
    "acute_lsoas   = _load_site_codes(ACUTE_CSV)\n",
    "\n",
    "# Response mins: min(station→LSOA) per demand LSOA\n",
    "g = (\n",
    "    travel.loc[travel[\"origin_lsoa\"].isin(station_lsoas)]\n",
    "    .groupby(\"dest_lsoa\")[\"time_car_min\"]\n",
    "    .min()\n",
    ")\n",
    "t_resp_min = g.reindex(lsoa_index).astype(\"float32\").rename(\"t_resp_min\")\n",
    "\n",
    "# Conveyance mins: min(LSOA→acute) per demand LSOA\n",
    "g = (\n",
    "    travel.loc[travel[\"dest_lsoa\"].isin(acute_lsoas)]\n",
    "    .groupby(\"origin_lsoa\")[\"time_car_min\"]\n",
    "    .min()\n",
    ")\n",
    "t_conv_min = g.reindex(lsoa_index).astype(\"float32\").rename(\"t_conv_min\")\n",
    "\n",
    "# Persist baseline min-times + labels for 02b/02c\n",
    "BASELINE_NPZ = MATRICES / \"baseline_min_times.npz\"\n",
    "np.savez_compressed(\n",
    "    BASELINE_NPZ,\n",
    "    t_resp_base=t_resp_min.to_numpy(dtype=\"float32\"),\n",
    "    t_conv_base=t_conv_min.to_numpy(dtype=\"float32\"),\n",
    "    lsoa_codes=np.array(lsoa_index, dtype=object),\n",
    "    station_lsoas=station_lsoas,\n",
    "    acute_lsoas=acute_lsoas,\n",
    ")\n",
    "_save_prov(BASELINE_NPZ, {\"source\": \"computed_from_travel\"})\n",
    "_ok(\n",
    "    f\"Baseline mins → {BASELINE_NPZ.name} | \"\n",
    "    f\"resp(mean)={float(t_resp_min.mean()):.1f} | \"\n",
    "    f\"conv(mean)={float(t_conv_min.mean()):.1f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eae21d07-c44b-41d1-b6b3-870de065b26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] Response coverage counts: {7: 106, 15: 213, 18: 253, 40: 334}\n",
      "[OK] Monotonicity confirmed.\n"
     ]
    }
   ],
   "source": [
    "# Step 5 — Coverage flags + monotonicity\n",
    "\n",
    "pop = (population.set_index(\"lsoa_code\")[\"population\"]\n",
    "       .astype(\"float32\").reindex(lsoa_index))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"lsoa_code\": lsoa_index,\n",
    "    \"population\": pop.values,\n",
    "    \"t_resp_min\": t_resp_min.values,\n",
    "    \"t_conv_min\": t_conv_min.values,\n",
    "}).set_index(\"lsoa_code\")\n",
    "\n",
    "for t in RESPONSE_THRESHOLDS:\n",
    "    df[f\"covered_T{t}\"] = df[\"t_resp_min\"] <= t\n",
    "for t in SCENE_TO_AE_THRESHOLDS:\n",
    "    df[f\"conv_T{t}\"] = df[\"t_conv_min\"] <= t\n",
    "\n",
    "covered_counts = {t: int(df[f\"covered_T{t}\"].sum()) for t in RESPONSE_THRESHOLDS}\n",
    "print(\"[CHECK] Response coverage counts:\", covered_counts)\n",
    "_check_monotone(covered_counts)\n",
    "_ok(\"Monotonicity confirmed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1962d81-ad87-4884-a3bf-0020a331eb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wrote coverage_summary.csv | rows=7\n",
      "[OK] Wrote lsoa_flags_baseline.gpkg (layer='lsoa_flags_baseline')\n",
      "\n",
      "=== ACCEPTANCE ===\n",
      "Universe LSOAs: 336\n",
      "Population sum: 570604\n",
      "Coverage counts: {7: 106, 15: 213, 18: 253, 40: 334}\n",
      "Monotone: True\n"
     ]
    }
   ],
   "source": [
    "# Step 6 — KPI summary CSV (+ map layer if geometry available)\n",
    "\n",
    "def _summary_overall(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    tot_pop = int(frame[\"population\"].sum())\n",
    "    for t in RESPONSE_THRESHOLDS:\n",
    "        cov_pop = int(frame.loc[frame[f\"covered_T{t}\"], \"population\"].sum())\n",
    "        rows.append({\"dimension\":\"overall\",\"group\":\"ALL\",\"metric\":f\"resp_T{t}\",\n",
    "                     \"covered_pop\":cov_pop,\"total_pop\":tot_pop,\n",
    "                     \"covered_pct\": round(100*cov_pop/tot_pop, 2) if tot_pop else 0.0})\n",
    "    for t in SCENE_TO_AE_THRESHOLDS:\n",
    "        cov_pop = int(frame.loc[frame[f\"conv_T{t}\"], \"population\"].sum())\n",
    "        rows.append({\"dimension\":\"overall\",\"group\":\"ALL\",\"metric\":f\"conv_T{t}\",\n",
    "                     \"covered_pop\":cov_pop,\"total_pop\":tot_pop,\n",
    "                     \"covered_pct\": round(100*cov_pop/tot_pop, 2) if tot_pop else 0.0})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "coverage_summary = _summary_overall(df)\n",
    "COVERAGE_CSV = TABLES / \"coverage_summary.csv\"\n",
    "coverage_summary.to_csv(COVERAGE_CSV, index=False)\n",
    "_save_prov(COVERAGE_CSV, {\"sources\":[\"population_by_lsoa.parquet\",\"baseline_min_times.npz\"]})\n",
    "_ok(f\"Wrote {COVERAGE_CSV.name} | rows={len(coverage_summary):,}\")\n",
    "\n",
    "# Equity splits can be added later; for now we persist a mapping layer if geometry exists\n",
    "if hasattr(lsoa_g, \"geometry\"):\n",
    "    FLAGS_GPKG = MAPS / \"lsoa_flags_baseline.gpkg\"\n",
    "    cols = [\"population\",\"t_resp_min\",\"t_conv_min\"] + [f\"covered_T{t}\" for t in RESPONSE_THRESHOLDS]\n",
    "    gout = lsoa_g.merge(df[cols], left_on=\"lsoa_code\", right_index=True, how=\"right\")\n",
    "    gout = gpd.GeoDataFrame(gout, geometry=gout.geometry, crs=getattr(lsoa_g, \"crs\", None))\n",
    "    gout.to_file(FLAGS_GPKG, layer=\"lsoa_flags_baseline\", driver=\"GPKG\")\n",
    "    _save_prov(FLAGS_GPKG, {\"layer\":\"lsoa_flags_baseline\"})\n",
    "    _ok(f\"Wrote {FLAGS_GPKG.name} (layer='lsoa_flags_baseline')\")\n",
    "else:\n",
    "    _warn(\"No geometry available; skipped map GPKG export.\")\n",
    "\n",
    "# Compact acceptance\n",
    "print(\"\\n=== ACCEPTANCE ===\")\n",
    "print(\"Universe LSOAs:\", len(lsoa_index))\n",
    "print(\"Population sum:\", int(population['population'].sum()))\n",
    "print(\"Coverage counts:\", covered_counts)\n",
    "print(\"Monotone:\", all(b >= a for a, b in zip(\n",
    "    [covered_counts[t] for t in sorted(RESPONSE_THRESHOLDS)],\n",
    "    [covered_counts[t] for t in sorted(RESPONSE_THRESHOLDS)][1:]\n",
    ")))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
